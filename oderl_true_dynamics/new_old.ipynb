{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/13/avcib1/unix/.pyenv/versions/3.7.7/envs/myproject/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from envs.ctcartpole import CTCartpole\n",
    "from envs.ctpendulum import CTPendulum\n",
    "from envs.ctacrobot import CTAcrobot\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchdiffeq import odeint\n",
    "from basic_mdl import basic_mdl\n",
    "from utils import *\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fully actuated Acrobot\n",
      "[-0.0025, 1.0]\n"
     ]
    }
   ],
   "source": [
    "env = CTAcrobot(dt=0.1, device='cuda', obs_trans=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh_ = nn.Tanh()\n",
    "\n",
    "def final_activation(env, a):\n",
    "    return tanh_(a) * env.act_rng\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, env, nl=2, nn=200, act='relu'):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        self.act = act\n",
    "        self._g = basic_mdl(env.n, env.m, n_hid_layers=nl, act=act, n_hidden=nn, dropout=0.0)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self, w=1.0):\n",
    "        self._g.reset_parameters(w)\n",
    "    \n",
    "    def forward(self, s, t):\n",
    "        s = s.to(self.env.device)  # Ensure s is on the correct device\n",
    "        a = self._g(s)\n",
    "        return final_activation(self.env, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as distributions\n",
    "\n",
    "class StochasticPolicy(nn.Module):\n",
    "    def __init__(self, env, nl=2, nn=200, act='relu'):\n",
    "        super(StochasticPolicy, self).__init__()\n",
    "        self.env = env\n",
    "\n",
    "        # Ensure env.m is a tensor (assuming it's an integer initially)\n",
    "        if not isinstance(env.m, torch.Tensor):\n",
    "            self.env.m = torch.tensor(env.m)\n",
    "\n",
    "        # Initialize the mean network (reuse your existing Policy._g)\n",
    "        self._g = basic_mdl(env.n, env.m, n_hid_layers=nl, act=act, n_hidden=nn, dropout=0.0)\n",
    "\n",
    "        # Create separate networks for mean and log std\n",
    "        self._mean_layer = nn.Linear(self._g.out_features, env.m)  # Output size is number of actions\n",
    "        self._log_std_layer = nn.Linear(self._g.out_features, env.m)  # Output size is number of actions for log std\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self, w=0.1):\n",
    "        self._g.reset_parameters(w)\n",
    "        nn.init.xavier_uniform_(self._mean_layer.weight)\n",
    "        nn.init.constant_(self._mean_layer.bias, 0)\n",
    "        nn.init.constant_(self._log_std_layer.bias, -0.5)  # Initialize log std with a small value\n",
    "\n",
    "    def forward(self, s, t):\n",
    "        s = s.to(self.env.device)  # Ensure s is on the correct device\n",
    "        x = self._g(s)  # Pass through the base network\n",
    "\n",
    "        # Separate predictions for mean and log std\n",
    "        mean = self._mean_layer(x)\n",
    "        log_std = self._log_std_layer(x)\n",
    "\n",
    "        # Ensure the log_std is expanded to match the mean's dimensions\n",
    "        log_std = log_std.expand_as(mean)\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        # Create a Normal distribution with the mean and std\n",
    "        dist = distributions.Normal(mean, std)\n",
    "\n",
    "        # Sample action from the distribution using the reparameterization trick\n",
    "        a = dist.rsample()\n",
    "        action = final_activation(self.env, a)\n",
    "\n",
    "        return action, dist\n",
    "\n",
    "    def get_action(self, s, t):\n",
    "        action, _ = self.forward(s, t)\n",
    "        return action\n",
    "\n",
    "    def get_log_prob(self, s, t, action):\n",
    "        _, dist = self.forward(s, t)\n",
    "        log_prob = dist.log_prob(action).sum(-1)  # Sum over the action dimensions\n",
    "        return log_prob\n",
    "    \n",
    "policy_nn = StochasticPolicy(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cartpole Swingup = FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma):\n",
    "    discounted_r = torch.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size(-1))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "# random seed\n",
    "\n",
    "# Set seed for reproducibility\n",
    "device = env.device\n",
    "\n",
    "# Initialize policy network\n",
    "policy_nn = Policy(env)\n",
    "policy_nn.to(device)\n",
    "\n",
    "# Initialize value network\n",
    "us_V = basic_mdl(env.n, 1, n_hid_layers=2, act=\"tanh\", n_hidden=200)\n",
    "us_V.reset_parameters()\n",
    "us_V.to(device)\n",
    "\n",
    "# Initialize optimizers\n",
    "policy_opt = optim.Adam(policy_nn.parameters(), lr=0.001)\n",
    "us_V_opt = optim.Adam(us_V.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_episodes =250\n",
    "num_steps = 20\n",
    "num_rounds = 50\n",
    "gamma = 0.99\n",
    "tau = 2.0\n",
    "\n",
    "\n",
    "flag = True\n",
    "\n",
    "for rounds in range(num_rounds):\n",
    "\n",
    "    # if mean_reward > 0.2 decreased the learning rate\n",
    "\n",
    "    rewards,opt_objs = [],[]\n",
    "    for episode in range(num_episodes):\n",
    "        if episode%50==0:\n",
    "            Vtarget = copy.deepcopy(us_V)\n",
    "\n",
    "\n",
    "        initial_observations = [env.reset() for _ in range(50)]\n",
    "        s0 = torch.stack([env.obs2state(torch.tensor(obs, device=device)) for obs in initial_observations])\n",
    "        ts = env.build_time_grid(num_steps).to(device)\n",
    "        policy_opt.zero_grad()\n",
    "\n",
    "        st, at, rt, ts  = env.integrate_system(T=num_steps, g=policy_nn, s0=s0, N=1)\n",
    "        # print(rt.shape)\n",
    "        rew_int  = rt[:,-1].mean(0)  # N\n",
    "        # print(rew_int.shape)\n",
    "    \n",
    "        # print(st.shape)\n",
    "        st = torch.cat([st]*5) if st.shape[0]==1 else st\n",
    "        # print(st.shape)\n",
    "        ts = ts[0]\n",
    "        gammas = (-ts/tau).exp() # H\n",
    "        # print(us_V(st.contiguous()).shape)\n",
    "        V_st_gam = us_V(st.contiguous())[:,1:,0] * gammas[1:] # L,N,H-1\n",
    "        # print(V_st_gam.shape)\n",
    "        V_const = min(rounds/5.0,1)\n",
    "        # print((V_const*V_st_gam).shape)\n",
    "        # print(rt[:,1:].shape)\n",
    "        n_step_returns = rt[:,:,1:].squeeze(0) + V_const*V_st_gam # ---> n_step_returns[:,:,k] is the sum in (5)\n",
    "        # print(\"nstep\",n_step_returns.shape)\n",
    "        optimized_returns = n_step_returns.mean(-1) # L,N\n",
    "        # print(\"optimized\",optimized_returns.shape)\n",
    "        # print(optimized_returns)\n",
    "        mean_cost = -optimized_returns.mean()\n",
    "        # print(mean_cost.shape)\n",
    "        # print(mean_cost)\n",
    "        mean_cost.backward()\n",
    "        grad_norm = torch.norm(flatten_([p.grad for p in policy_nn.parameters()])).item()\n",
    "        policy_opt.step()\n",
    "\n",
    "        rewards.append(rew_int.mean().item()/2.0)\n",
    "        opt_objs.append(mean_cost.mean().item())\n",
    "        print_log = 'Round: {:4d}/{:<4d}, Iter:{:4d}/{:<4d},  opt. target:{:.3f}  mean reward:{:.3f}  '\\\n",
    "                .format(rounds,num_rounds, episode, num_episodes, np.mean(opt_objs), np.mean(rewards)) + \\\n",
    "                'H={:.2f},  grad_norm={:.3f},  '.format(2.0,grad_norm)\n",
    "        \n",
    "        # if rounds > 0:\n",
    "        #     if np.mean(rewards) > 0.15 and flag:\n",
    "        #         policy_opt = optim.Adam(policy_nn.parameters(), lr=0.001)\n",
    "        #         us_V_opt = optim.Adam(us_V.parameters(), lr=0.001)\n",
    "        #         flag = False\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # regress all intermediate values\n",
    "            # print(\"Regressing all intermediate values\")\n",
    "            # print(st.detach().contiguous().shape)\n",
    "            last_states = st.detach().contiguous()[:,1:,:] # L,N,T-1,n\n",
    "            # print(\"last\",last_states.shape)\n",
    "            last_values = Vtarget(last_states).squeeze(-1)\n",
    "            # print(\"last_val\",last_values.shape)\n",
    "            # print(((-ts[1:]/tau).exp()*last_values ).shape)\n",
    "            # print((rt[:,1:,:].squeeze()).shape)\n",
    "            Vtargets = rt[:,:,1:].squeeze(0) + (-ts[1:]/tau).exp()*last_values # L,N,T-1\n",
    "            # print(\"Vvv\",Vtargets.shape)\n",
    "            Vtargets = Vtargets.mean(-1)\n",
    "            # print(Vtargets.shape)\n",
    "        mean_val_err = 0\n",
    "\n",
    "\n",
    "        for inner_iter in range(5):\n",
    "            us_V_opt.zero_grad()\n",
    "            # print(\"USV\",us_V(s0).squeeze(-1).shape)\n",
    "            # print(s0.shape)\n",
    "            td_error = us_V(s0).squeeze(-1) - Vtargets # L,N\n",
    "            td_error = torch.mean(td_error**2)\n",
    "            # print(td_error)\n",
    "            td_error.backward()\n",
    "            mean_val_err += td_error.item() / 10\n",
    "            if inner_iter==0:\n",
    "                first_val_err = td_error.item()\n",
    "            us_V_opt.step()\n",
    "\n",
    "        if episode%(num_episodes//10)==0:\n",
    "            print(print_log)\n",
    "            # Save the model indicate rounds and and episode\n",
    "            if not os.path.exists('models/'):\n",
    "                os.makedirs('models/')\n",
    "\n",
    "            if not os.path.exists('models/policy6'):\n",
    "                os.makedirs('models/policy6')\n",
    "            \n",
    "            if not os.path.exists('models/value6'):\n",
    "                os.makedirs('models/value6')\n",
    "\n",
    "            \n",
    "\n",
    "            print(\"Saving model at round {} and episode {}\".format(rounds, episode))\n",
    "            torch.save(policy_nn.state_dict(), 'models/policy6/round_{}_episode_{}.pt'.format(rounds, episode))\n",
    "            torch.save(us_V.state_dict(), 'models/value6/round_{}_episode_{}.pt'.format(rounds, episode))\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Htest,Ntest,Tup = 30,10,int(3.0/0.1)\n",
    "        initial_observations = [env.reset() for _ in range(10)]\n",
    "        s0 = torch.stack([env.obs2state(torch.tensor(obs, device=device)) for obs in initial_observations])\n",
    "        test_states, test_actions, test_rewards,_ = env.integrate_system(T=200, s0=s0, g=policy_nn)\n",
    "        \n",
    "        true_test_rewards = test_rewards[...,Tup:].mean().item()\n",
    "        print(\"True test rewards: \", true_test_rewards)\n",
    "        \n",
    "        if true_test_rewards > 0.9:\n",
    "            print(\"Test rewards > 0.9. Training complete...\")\n",
    "\n",
    "            break\n",
    "\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run saved policy on the environment\n",
    "import time\n",
    "device = 'cuda'\n",
    "rounds = 2\n",
    "episode = 200\n",
    "policy_nn = Policy(env)\n",
    "policy_nn.load_state_dict(torch.load('models/policy6/round_{}_episode_{}.pt'.format(rounds, episode)))\n",
    "policy_nn.to(device)\n",
    "policy_nn.eval()\n",
    "\n",
    "us_V = basic_mdl(env.n, env.m, n_hid_layers=2, act=\"tanh\", n_hidden=200)\n",
    "us_V.load_state_dict(torch.load('models/value6/round_{}_episode_{}.pt'.format(rounds, episode)))\n",
    "us_V.to(device)\n",
    "us_V.eval()\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    Htest,Ntest,Tup = 30,10,int(3.0/0.1)\n",
    "    initial_observations = [env.reset() for _ in range(10)]\n",
    "    s0 = torch.stack([env.obs2state(torch.tensor(obs, device=device)) for obs in initial_observations])\n",
    "    test_states,test_actions, test_rewards,_ = env.integrate_system(T=500, s0=s0, g=policy_nn)\n",
    "    # print(test_rewards)\n",
    "    # print(test_actions)\n",
    "    true_test_rewards = test_rewards[...,Tup:].mean().item()\n",
    "    print('True test reward: {:.3f}'.format(true_test_rewards))\n",
    "    for step in range(500):\n",
    "        observation = test_states[6, step].cpu().numpy()\n",
    "        env.set_state_(observation)\n",
    "        time.sleep(0.01)\n",
    "\n",
    "        img = env.render(mode='rgb_array')\n",
    "\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acrobot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:    0/50  , Iter:   0/50  ,  opt. target:0.001  mean reward:-0.000  H=2.00,  grad_norm=0.002,  \n",
      "Saving model at round 0 and episode 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_703304/3871478475.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# print(mean_cost.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# print(mean_cost)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mmean_cost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolicy_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mpolicy_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.7/envs/myproject/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.7/envs/myproject/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "def discount_rewards(r, gamma):\n",
    "    discounted_r = torch.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size(-1))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "# random seed\n",
    "\n",
    "# Set seed for reproducibility\n",
    "device = env.device\n",
    "\n",
    "# Initialize policy network\n",
    "policy_nn = Policy(env)\n",
    "policy_nn.to(device)\n",
    "\n",
    "# Initialize value network\n",
    "us_V = basic_mdl(env.n, 1, n_hid_layers=2, act=\"tanh\", n_hidden=200)\n",
    "us_V.reset_parameters()\n",
    "us_V.to(device)\n",
    "\n",
    "# Initialize optimizers\n",
    "policy_opt = optim.Adam(policy_nn.parameters(), lr=0.001)\n",
    "us_V_opt = optim.Adam(us_V.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_episodes =50\n",
    "num_steps = 20\n",
    "num_rounds = 50\n",
    "gamma = 0.99\n",
    "tau = 2.0\n",
    "\n",
    "\n",
    "flag = True\n",
    "\n",
    "for rounds in range(num_rounds):\n",
    "\n",
    "    # if mean_reward > 0.2 decreased the learning rate\n",
    "\n",
    "\n",
    "    rewards,opt_objs = [],[]\n",
    "    for episode in range(num_episodes):\n",
    "        if episode%50==0:\n",
    "            Vtarget = copy.deepcopy(us_V)\n",
    "\n",
    "\n",
    "        initial_observations = [env.reset() for _ in range(50)]\n",
    "        s0 = torch.stack([env.obs2state(torch.tensor(obs, device=device)) for obs in initial_observations])\n",
    "        ts = env.build_time_grid(num_steps).to(device)\n",
    "        policy_opt.zero_grad()\n",
    "\n",
    "        st, at, rt, ts  = env.integrate_system(T=num_steps, g=policy_nn, s0=s0, N=1)\n",
    "\n",
    "        rew_int  = rt[:,-1].mean(0)  # N\n",
    "        # print(rew_int.shape)\n",
    "    \n",
    "        # print(st.shape)\n",
    "        st = torch.cat([st]*5) if st.shape[0]==1 else st\n",
    "        # print(st.shape)\n",
    "        ts = ts[0]\n",
    "        gammas = (-ts/tau).exp() # H\n",
    "        # print(us_V(st.contiguous()).shape)\n",
    "        V_st_gam = us_V(st.contiguous())[:,1:,0] * gammas[1:] # L,N,H-1\n",
    "        # print(V_st_gam.shape)\n",
    "        V_const = min(rounds/5.0,1)\n",
    "        # print((V_const*V_st_gam).shape)\n",
    "        # print(rt[:,1:].shape)\n",
    "        n_step_returns = rt[:,1:] + V_const*V_st_gam # ---> n_step_returns[:,:,k] is the sum in (5)\n",
    "        # print(\"nstep\",n_step_returns.shape)\n",
    "        optimized_returns = n_step_returns.mean(-1) # L,N\n",
    "        # print(\"optimized\",optimized_returns.shape)\n",
    "        # print(optimized_returns)\n",
    "        mean_cost = -optimized_returns.mean()\n",
    "        # print(mean_cost.shape)\n",
    "        # print(mean_cost)\n",
    "        mean_cost.backward()\n",
    "        grad_norm = torch.norm(flatten_([p.grad for p in policy_nn.parameters()])).item()\n",
    "        policy_opt.step()\n",
    "\n",
    "        rewards.append(rew_int.mean().item()/2.0)\n",
    "        opt_objs.append(mean_cost.mean().item())\n",
    "        print_log = 'Round: {:4d}/{:<4d}, Iter:{:4d}/{:<4d},  opt. target:{:.3f}  mean reward:{:.3f}  '\\\n",
    "                .format(rounds,num_rounds, episode, num_episodes, np.mean(opt_objs), np.mean(rewards)) + \\\n",
    "                'H={:.2f},  grad_norm={:.3f},  '.format(2.0,grad_norm)\n",
    "        \n",
    "        # if rounds > 0:\n",
    "        #     if np.mean(rewards) > 0.15 and flag:\n",
    "        #         policy_opt = optim.Adam(policy_nn.parameters(), lr=0.001)\n",
    "        #         us_V_opt = optim.Adam(us_V.parameters(), lr=0.001)\n",
    "        #         flag = False\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # regress all intermediate values\n",
    "            # print(\"Regressing all intermediate values\")\n",
    "            # print(st.detach().contiguous().shape)\n",
    "            last_states = st.detach().contiguous()[:,1:,:] # L,N,T-1,n\n",
    "            # print(\"last\",last_states.shape)\n",
    "            last_values = Vtarget(last_states).squeeze(-1)\n",
    "            # print(\"last_val\",last_values.shape)\n",
    "            # print(((-ts[1:]/tau).exp()*last_values ).shape)\n",
    "            # print((rt[:,1:,:].squeeze()).shape)\n",
    "            Vtargets = rt[:,1:].squeeze() + (-ts[1:]/tau).exp()*last_values # L,N,T-1\n",
    "            # print(\"Vvv\",Vtargets.shape)\n",
    "            Vtargets = Vtargets.mean(-1)\n",
    "            # print(Vtargets.shape)\n",
    "        mean_val_err = 0\n",
    "\n",
    "\n",
    "        for inner_iter in range(5):\n",
    "            us_V_opt.zero_grad()\n",
    "            # print(\"USV\",us_V(s0).squeeze(-1).shape)\n",
    "            # print(s0.shape)\n",
    "            td_error = us_V(s0).squeeze(-1) - Vtargets # L,N\n",
    "            td_error = torch.mean(td_error**2)\n",
    "            # print(td_error)\n",
    "            td_error.backward()\n",
    "            mean_val_err += td_error.item() / 10\n",
    "            if inner_iter==0:\n",
    "                first_val_err = td_error.item()\n",
    "            us_V_opt.step()\n",
    "\n",
    "        if episode%(num_episodes//5)==0:\n",
    "            print(print_log)\n",
    "            # Save the model indicate rounds and and episode\n",
    "            if not os.path.exists('models/'):\n",
    "                os.makedirs('models/')\n",
    "\n",
    "            if not os.path.exists('models/policy7'):\n",
    "                os.makedirs('models/policy7')\n",
    "            \n",
    "            if not os.path.exists('models/value7'):\n",
    "                os.makedirs('models/value7')\n",
    "\n",
    "            \n",
    "\n",
    "            print(\"Saving model at round {} and episode {}\".format(rounds, episode))\n",
    "            torch.save(policy_nn.state_dict(), 'models/policy7/round_{}_episode_{}.pt'.format(rounds, episode))\n",
    "            torch.save(us_V.state_dict(), 'models/value7/round_{}_episode_{}.pt'.format(rounds, episode))\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Htest,Ntest,Tup = 30,10,int(3.0/0.1)\n",
    "        initial_observations = [env.reset() for _ in range(10)]\n",
    "        s0 = torch.stack([env.obs2state(torch.tensor(obs, device=device)) for obs in initial_observations])\n",
    "        test_states, test_actions, test_rewards,_ = env.integrate_system(T=200, s0=s0, g=policy_nn)\n",
    "        \n",
    "        true_test_rewards = test_rewards[...,Tup:].mean().item()\n",
    "        print(\"True test rewards: \", true_test_rewards)\n",
    "        for step in range(200):\n",
    "            observation = test_states[0, step].cpu().numpy()\n",
    "            env.set_state_(observation)\n",
    "            time.sleep(0.01)\n",
    "            img = env.render(mode='rgb_array')\n",
    "        \n",
    "        if true_test_rewards > 0.9:\n",
    "            print(\"Test rewards > 0.9. Training complete...\")\n",
    "\n",
    "            break\n",
    "\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cartpole Swingup = TRUE and Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbavci\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/m/home/home1/13/avcib1/unix/code/msoderl/playground/wandb/run-20240817_204216-9dmzdjtu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bavci/swingup_ctpole/runs/9dmzdjtu' target=\"_blank\">azure-monkey-7</a></strong> to <a href='https://wandb.ai/bavci/swingup_ctpole' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bavci/swingup_ctpole' target=\"_blank\">https://wandb.ai/bavci/swingup_ctpole</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bavci/swingup_ctpole/runs/9dmzdjtu' target=\"_blank\">https://wandb.ai/bavci/swingup_ctpole/runs/9dmzdjtu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/13/avcib1/unix/.pyenv/versions/3.7.7/envs/myproject/lib/python3.7/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:    0/50  , Iter:   0/250 ,  opt. target:-0.192  mean reward:0.069  H=2.00,  grad_norm=0.896,  \n",
      "Saving model at round 0 and episode 0\n",
      "Round:    0/50  , Iter:  50/250 ,  opt. target:-0.233  mean reward:0.221  H=2.00,  grad_norm=0.070,  \n",
      "Saving model at round 0 and episode 50\n",
      "Round:    0/50  , Iter: 100/250 ,  opt. target:-0.235  mean reward:0.228  H=2.00,  grad_norm=0.014,  \n",
      "Saving model at round 0 and episode 100\n",
      "Round:    0/50  , Iter: 150/250 ,  opt. target:-0.235  mean reward:0.231  H=2.00,  grad_norm=0.015,  \n",
      "Saving model at round 0 and episode 150\n",
      "Round:    0/50  , Iter: 200/250 ,  opt. target:-0.236  mean reward:0.232  H=2.00,  grad_norm=0.015,  \n",
      "Saving model at round 0 and episode 200\n",
      "True test rewards:  0.23690665922598922\n",
      "Round:    1/50  , Iter:   0/250 ,  opt. target:-0.302  mean reward:0.237  H=2.00,  grad_norm=0.027,  \n",
      "Saving model at round 1 and episode 0\n",
      "Round:    1/50  , Iter:  50/250 ,  opt. target:-0.308  mean reward:0.237  H=2.00,  grad_norm=0.051,  \n",
      "Saving model at round 1 and episode 50\n",
      "Round:    1/50  , Iter: 100/250 ,  opt. target:-0.309  mean reward:0.237  H=2.00,  grad_norm=0.029,  \n",
      "Saving model at round 1 and episode 100\n",
      "Round:    1/50  , Iter: 150/250 ,  opt. target:-0.311  mean reward:0.237  H=2.00,  grad_norm=0.019,  \n",
      "Saving model at round 1 and episode 150\n",
      "Round:    1/50  , Iter: 200/250 ,  opt. target:-0.312  mean reward:0.237  H=2.00,  grad_norm=0.008,  \n",
      "Saving model at round 1 and episode 200\n",
      "True test rewards:  0.20892799722922872\n",
      "Round:    2/50  , Iter:   0/250 ,  opt. target:-0.395  mean reward:0.237  H=2.00,  grad_norm=0.014,  \n",
      "Saving model at round 2 and episode 0\n",
      "Round:    2/50  , Iter:  50/250 ,  opt. target:-0.397  mean reward:0.237  H=2.00,  grad_norm=0.026,  \n",
      "Saving model at round 2 and episode 50\n",
      "Round:    2/50  , Iter: 100/250 ,  opt. target:-0.396  mean reward:0.237  H=2.00,  grad_norm=0.012,  \n",
      "Saving model at round 2 and episode 100\n",
      "Round:    2/50  , Iter: 150/250 ,  opt. target:-0.396  mean reward:0.237  H=2.00,  grad_norm=0.002,  \n",
      "Saving model at round 2 and episode 150\n",
      "Round:    2/50  , Iter: 200/250 ,  opt. target:-0.396  mean reward:0.237  H=2.00,  grad_norm=0.019,  \n",
      "Saving model at round 2 and episode 200\n",
      "True test rewards:  0.004140404798628875\n",
      "Round:    3/50  , Iter:   0/250 ,  opt. target:-0.474  mean reward:0.235  H=2.00,  grad_norm=0.054,  \n",
      "Saving model at round 3 and episode 0\n",
      "Round:    3/50  , Iter:  50/250 ,  opt. target:-0.514  mean reward:0.301  H=2.00,  grad_norm=0.323,  \n",
      "Saving model at round 3 and episode 50\n",
      "Round:    3/50  , Iter: 100/250 ,  opt. target:-0.588  mean reward:0.544  H=2.00,  grad_norm=0.237,  \n",
      "Saving model at round 3 and episode 100\n",
      "Round:    3/50  , Iter: 150/250 ,  opt. target:-0.612  mean reward:0.643  H=2.00,  grad_norm=0.367,  \n",
      "Saving model at round 3 and episode 150\n",
      "Round:    3/50  , Iter: 200/250 ,  opt. target:-0.627  mean reward:0.696  H=2.00,  grad_norm=0.235,  \n",
      "Saving model at round 3 and episode 200\n",
      "True test rewards:  -0.01287920113118598\n",
      "Round:    4/50  , Iter:   0/250 ,  opt. target:-0.720  mean reward:0.903  H=2.00,  grad_norm=0.759,  \n",
      "Saving model at round 4 and episode 0\n",
      "Round:    4/50  , Iter:  50/250 ,  opt. target:-0.724  mean reward:0.816  H=2.00,  grad_norm=0.478,  \n",
      "Saving model at round 4 and episode 50\n",
      "Round:    4/50  , Iter: 100/250 ,  opt. target:-0.729  mean reward:0.815  H=2.00,  grad_norm=0.493,  \n",
      "Saving model at round 4 and episode 100\n",
      "Round:    4/50  , Iter: 150/250 ,  opt. target:-0.736  mean reward:0.824  H=2.00,  grad_norm=0.653,  \n",
      "Saving model at round 4 and episode 150\n",
      "Round:    4/50  , Iter: 200/250 ,  opt. target:-0.733  mean reward:0.825  H=2.00,  grad_norm=0.165,  \n",
      "Saving model at round 4 and episode 200\n",
      "True test rewards:  -0.009650088804804642\n",
      "Round:    5/50  , Iter:   0/250 ,  opt. target:-0.770  mean reward:0.744  H=2.00,  grad_norm=0.205,  \n",
      "Saving model at round 5 and episode 0\n",
      "Round:    5/50  , Iter:  50/250 ,  opt. target:-0.754  mean reward:0.809  H=2.00,  grad_norm=0.014,  \n",
      "Saving model at round 5 and episode 50\n",
      "Round:    5/50  , Iter: 100/250 ,  opt. target:-0.750  mean reward:0.826  H=2.00,  grad_norm=0.188,  \n",
      "Saving model at round 5 and episode 100\n",
      "Round:    5/50  , Iter: 150/250 ,  opt. target:-0.738  mean reward:0.831  H=2.00,  grad_norm=0.189,  \n",
      "Saving model at round 5 and episode 150\n",
      "Round:    5/50  , Iter: 200/250 ,  opt. target:-0.729  mean reward:0.843  H=2.00,  grad_norm=0.139,  \n",
      "Saving model at round 5 and episode 200\n",
      "True test rewards:  0.04900886508967436\n",
      "Round:    6/50  , Iter:   0/250 ,  opt. target:-0.745  mean reward:0.942  H=2.00,  grad_norm=0.447,  \n",
      "Saving model at round 6 and episode 0\n",
      "Round:    6/50  , Iter:  50/250 ,  opt. target:-0.768  mean reward:0.926  H=2.00,  grad_norm=0.330,  \n",
      "Saving model at round 6 and episode 50\n",
      "Round:    6/50  , Iter: 100/250 ,  opt. target:-0.747  mean reward:0.922  H=2.00,  grad_norm=0.281,  \n",
      "Saving model at round 6 and episode 100\n",
      "Round:    6/50  , Iter: 150/250 ,  opt. target:-0.745  mean reward:0.924  H=2.00,  grad_norm=0.357,  \n",
      "Saving model at round 6 and episode 150\n",
      "Round:    6/50  , Iter: 200/250 ,  opt. target:-0.736  mean reward:0.924  H=2.00,  grad_norm=0.114,  \n",
      "Saving model at round 6 and episode 200\n",
      "True test rewards:  0.049907150172296404\n",
      "Round:    7/50  , Iter:   0/250 ,  opt. target:-0.731  mean reward:0.938  H=2.00,  grad_norm=0.176,  \n",
      "Saving model at round 7 and episode 0\n",
      "Round:    7/50  , Iter:  50/250 ,  opt. target:-0.772  mean reward:0.927  H=2.00,  grad_norm=0.067,  \n",
      "Saving model at round 7 and episode 50\n",
      "Round:    7/50  , Iter: 100/250 ,  opt. target:-0.759  mean reward:0.927  H=2.00,  grad_norm=0.255,  \n",
      "Saving model at round 7 and episode 100\n",
      "Round:    7/50  , Iter: 150/250 ,  opt. target:-0.747  mean reward:0.926  H=2.00,  grad_norm=0.328,  \n",
      "Saving model at round 7 and episode 150\n",
      "Round:    7/50  , Iter: 200/250 ,  opt. target:-0.744  mean reward:0.927  H=2.00,  grad_norm=0.205,  \n",
      "Saving model at round 7 and episode 200\n",
      "True test rewards:  0.030558388061607832\n",
      "Round:    8/50  , Iter:   0/250 ,  opt. target:-0.720  mean reward:0.939  H=2.00,  grad_norm=0.064,  \n",
      "Saving model at round 8 and episode 0\n",
      "Round:    8/50  , Iter:  50/250 ,  opt. target:-0.700  mean reward:0.929  H=2.00,  grad_norm=0.324,  \n",
      "Saving model at round 8 and episode 50\n",
      "Round:    8/50  , Iter: 100/250 ,  opt. target:-0.701  mean reward:0.928  H=2.00,  grad_norm=0.082,  \n",
      "Saving model at round 8 and episode 100\n",
      "Round:    8/50  , Iter: 150/250 ,  opt. target:-0.714  mean reward:0.930  H=2.00,  grad_norm=0.248,  \n",
      "Saving model at round 8 and episode 150\n",
      "Round:    8/50  , Iter: 200/250 ,  opt. target:-0.716  mean reward:0.931  H=2.00,  grad_norm=0.208,  \n",
      "Saving model at round 8 and episode 200\n",
      "True test rewards:  0.018607162690676776\n",
      "Round:    9/50  , Iter:   0/250 ,  opt. target:-0.695  mean reward:0.954  H=2.00,  grad_norm=0.417,  \n",
      "Saving model at round 9 and episode 0\n",
      "Round:    9/50  , Iter:  50/250 ,  opt. target:-0.707  mean reward:0.930  H=2.00,  grad_norm=0.028,  \n",
      "Saving model at round 9 and episode 50\n",
      "Round:    9/50  , Iter: 100/250 ,  opt. target:-0.706  mean reward:0.932  H=2.00,  grad_norm=0.284,  \n",
      "Saving model at round 9 and episode 100\n",
      "Round:    9/50  , Iter: 150/250 ,  opt. target:-0.704  mean reward:0.932  H=2.00,  grad_norm=0.276,  \n",
      "Saving model at round 9 and episode 150\n",
      "Round:    9/50  , Iter: 200/250 ,  opt. target:-0.705  mean reward:0.934  H=2.00,  grad_norm=0.286,  \n",
      "Saving model at round 9 and episode 200\n",
      "True test rewards:  0.015495170352562183\n",
      "Round:   10/50  , Iter:   0/250 ,  opt. target:-0.684  mean reward:0.909  H=2.00,  grad_norm=0.207,  \n",
      "Saving model at round 10 and episode 0\n",
      "Round:   10/50  , Iter:  50/250 ,  opt. target:-0.668  mean reward:0.928  H=2.00,  grad_norm=0.424,  \n",
      "Saving model at round 10 and episode 50\n",
      "Round:   10/50  , Iter: 100/250 ,  opt. target:-0.669  mean reward:0.931  H=2.00,  grad_norm=0.083,  \n",
      "Saving model at round 10 and episode 100\n",
      "Round:   10/50  , Iter: 150/250 ,  opt. target:-0.666  mean reward:0.930  H=2.00,  grad_norm=0.120,  \n",
      "Saving model at round 10 and episode 150\n",
      "Round:   10/50  , Iter: 200/250 ,  opt. target:-0.667  mean reward:0.931  H=2.00,  grad_norm=0.020,  \n",
      "Saving model at round 10 and episode 200\n",
      "True test rewards:  0.011072218931665401\n",
      "Round:   11/50  , Iter:   0/250 ,  opt. target:-0.634  mean reward:0.888  H=2.00,  grad_norm=0.490,  \n",
      "Saving model at round 11 and episode 0\n",
      "Round:   11/50  , Iter:  50/250 ,  opt. target:-0.646  mean reward:0.926  H=2.00,  grad_norm=0.225,  \n",
      "Saving model at round 11 and episode 50\n",
      "Round:   11/50  , Iter: 100/250 ,  opt. target:-0.649  mean reward:0.929  H=2.00,  grad_norm=0.176,  \n",
      "Saving model at round 11 and episode 100\n",
      "Round:   11/50  , Iter: 150/250 ,  opt. target:-0.648  mean reward:0.930  H=2.00,  grad_norm=0.150,  \n",
      "Saving model at round 11 and episode 150\n",
      "Round:   11/50  , Iter: 200/250 ,  opt. target:-0.639  mean reward:0.929  H=2.00,  grad_norm=0.149,  \n",
      "Saving model at round 11 and episode 200\n",
      "True test rewards:  0.0097827550231694\n",
      "Round:   12/50  , Iter:   0/250 ,  opt. target:-0.578  mean reward:0.871  H=2.00,  grad_norm=0.337,  \n",
      "Saving model at round 12 and episode 0\n",
      "Round:   12/50  , Iter:  50/250 ,  opt. target:-0.552  mean reward:0.907  H=2.00,  grad_norm=0.284,  \n",
      "Saving model at round 12 and episode 50\n",
      "Round:   12/50  , Iter: 100/250 ,  opt. target:-0.567  mean reward:0.917  H=2.00,  grad_norm=0.051,  \n",
      "Saving model at round 12 and episode 100\n",
      "Round:   12/50  , Iter: 150/250 ,  opt. target:-0.571  mean reward:0.916  H=2.00,  grad_norm=0.582,  \n",
      "Saving model at round 12 and episode 150\n",
      "Round:   12/50  , Iter: 200/250 ,  opt. target:-0.568  mean reward:0.913  H=2.00,  grad_norm=0.250,  \n",
      "Saving model at round 12 and episode 200\n",
      "True test rewards:  0.005280235545129431\n",
      "Round:   13/50  , Iter:   0/250 ,  opt. target:-0.554  mean reward:0.870  H=2.00,  grad_norm=0.330,  \n",
      "Saving model at round 13 and episode 0\n",
      "Round:   13/50  , Iter:  50/250 ,  opt. target:-0.528  mean reward:0.873  H=2.00,  grad_norm=0.312,  \n",
      "Saving model at round 13 and episode 50\n",
      "Round:   13/50  , Iter: 100/250 ,  opt. target:-0.527  mean reward:0.890  H=2.00,  grad_norm=0.152,  \n",
      "Saving model at round 13 and episode 100\n",
      "Round:   13/50  , Iter: 150/250 ,  opt. target:-0.535  mean reward:0.898  H=2.00,  grad_norm=0.562,  \n",
      "Saving model at round 13 and episode 150\n",
      "Round:   13/50  , Iter: 200/250 ,  opt. target:-0.522  mean reward:0.887  H=2.00,  grad_norm=0.550,  \n",
      "Saving model at round 13 and episode 200\n",
      "True test rewards:  0.004109250420419826\n",
      "Round:   14/50  , Iter:   0/250 ,  opt. target:-0.468  mean reward:0.898  H=2.00,  grad_norm=0.378,  \n",
      "Saving model at round 14 and episode 0\n",
      "Round:   14/50  , Iter:  50/250 ,  opt. target:-0.440  mean reward:0.873  H=2.00,  grad_norm=0.344,  \n",
      "Saving model at round 14 and episode 50\n",
      "Round:   14/50  , Iter: 100/250 ,  opt. target:-0.443  mean reward:0.874  H=2.00,  grad_norm=0.433,  \n",
      "Saving model at round 14 and episode 100\n",
      "Round:   14/50  , Iter: 150/250 ,  opt. target:-0.448  mean reward:0.880  H=2.00,  grad_norm=0.261,  \n",
      "Saving model at round 14 and episode 150\n",
      "Round:   14/50  , Iter: 200/250 ,  opt. target:-0.449  mean reward:0.878  H=2.00,  grad_norm=0.261,  \n",
      "Saving model at round 14 and episode 200\n",
      "True test rewards:  -0.004906305518552231\n",
      "Round:   15/50  , Iter:   0/250 ,  opt. target:-0.419  mean reward:0.810  H=2.00,  grad_norm=0.756,  \n",
      "Saving model at round 15 and episode 0\n",
      "Round:   15/50  , Iter:  50/250 ,  opt. target:-0.425  mean reward:0.891  H=2.00,  grad_norm=0.246,  \n",
      "Saving model at round 15 and episode 50\n",
      "Round:   15/50  , Iter: 100/250 ,  opt. target:-0.433  mean reward:0.890  H=2.00,  grad_norm=0.087,  \n",
      "Saving model at round 15 and episode 100\n",
      "Round:   15/50  , Iter: 150/250 ,  opt. target:-0.434  mean reward:0.890  H=2.00,  grad_norm=0.451,  \n",
      "Saving model at round 15 and episode 150\n",
      "Round:   15/50  , Iter: 200/250 ,  opt. target:-0.430  mean reward:0.891  H=2.00,  grad_norm=0.437,  \n",
      "Saving model at round 15 and episode 200\n",
      "True test rewards:  0.005341838091049656\n",
      "Round:   16/50  , Iter:   0/250 ,  opt. target:-0.432  mean reward:0.879  H=2.00,  grad_norm=0.232,  \n",
      "Saving model at round 16 and episode 0\n",
      "Round:   16/50  , Iter:  50/250 ,  opt. target:-0.415  mean reward:0.867  H=2.00,  grad_norm=0.157,  \n",
      "Saving model at round 16 and episode 50\n",
      "Round:   16/50  , Iter: 100/250 ,  opt. target:-0.408  mean reward:0.876  H=2.00,  grad_norm=0.524,  \n",
      "Saving model at round 16 and episode 100\n",
      "Round:   16/50  , Iter: 150/250 ,  opt. target:-0.410  mean reward:0.882  H=2.00,  grad_norm=0.443,  \n",
      "Saving model at round 16 and episode 150\n",
      "Round:   16/50  , Iter: 200/250 ,  opt. target:-0.411  mean reward:0.886  H=2.00,  grad_norm=0.144,  \n",
      "Saving model at round 16 and episode 200\n",
      "True test rewards:  0.0006963307892792887\n",
      "Round:   17/50  , Iter:   0/250 ,  opt. target:-0.408  mean reward:0.859  H=2.00,  grad_norm=0.247,  \n",
      "Saving model at round 17 and episode 0\n",
      "Round:   17/50  , Iter:  50/250 ,  opt. target:-0.397  mean reward:0.891  H=2.00,  grad_norm=0.306,  \n",
      "Saving model at round 17 and episode 50\n",
      "Round:   17/50  , Iter: 100/250 ,  opt. target:-0.397  mean reward:0.891  H=2.00,  grad_norm=0.266,  \n",
      "Saving model at round 17 and episode 100\n",
      "Round:   17/50  , Iter: 150/250 ,  opt. target:-0.397  mean reward:0.893  H=2.00,  grad_norm=0.223,  \n",
      "Saving model at round 17 and episode 150\n",
      "Round:   17/50  , Iter: 200/250 ,  opt. target:-0.401  mean reward:0.895  H=2.00,  grad_norm=0.173,  \n",
      "Saving model at round 17 and episode 200\n",
      "True test rewards:  0.021020379388515593\n",
      "Round:   18/50  , Iter:   0/250 ,  opt. target:-0.430  mean reward:0.923  H=2.00,  grad_norm=0.555,  \n",
      "Saving model at round 18 and episode 0\n",
      "Round:   18/50  , Iter:  50/250 ,  opt. target:-0.400  mean reward:0.883  H=2.00,  grad_norm=0.167,  \n",
      "Saving model at round 18 and episode 50\n",
      "Round:   18/50  , Iter: 100/250 ,  opt. target:-0.394  mean reward:0.888  H=2.00,  grad_norm=0.087,  \n",
      "Saving model at round 18 and episode 100\n",
      "Round:   18/50  , Iter: 150/250 ,  opt. target:-0.393  mean reward:0.891  H=2.00,  grad_norm=0.373,  \n",
      "Saving model at round 18 and episode 150\n",
      "Round:   18/50  , Iter: 200/250 ,  opt. target:-0.394  mean reward:0.891  H=2.00,  grad_norm=0.103,  \n",
      "Saving model at round 18 and episode 200\n",
      "True test rewards:  0.018161369644840705\n",
      "Round:   19/50  , Iter:   0/250 ,  opt. target:-0.371  mean reward:0.870  H=2.00,  grad_norm=0.385,  \n",
      "Saving model at round 19 and episode 0\n",
      "Round:   19/50  , Iter:  50/250 ,  opt. target:-0.388  mean reward:0.884  H=2.00,  grad_norm=0.154,  \n",
      "Saving model at round 19 and episode 50\n",
      "Round:   19/50  , Iter: 100/250 ,  opt. target:-0.387  mean reward:0.880  H=2.00,  grad_norm=0.415,  \n",
      "Saving model at round 19 and episode 100\n",
      "Round:   19/50  , Iter: 150/250 ,  opt. target:-0.386  mean reward:0.886  H=2.00,  grad_norm=0.216,  \n",
      "Saving model at round 19 and episode 150\n",
      "Round:   19/50  , Iter: 200/250 ,  opt. target:-0.386  mean reward:0.884  H=2.00,  grad_norm=0.087,  \n",
      "Saving model at round 19 and episode 200\n",
      "True test rewards:  0.00493454902438521\n",
      "Round:   20/50  , Iter:   0/250 ,  opt. target:-0.380  mean reward:0.819  H=2.00,  grad_norm=0.282,  \n",
      "Saving model at round 20 and episode 0\n",
      "Round:   20/50  , Iter:  50/250 ,  opt. target:-0.369  mean reward:0.886  H=2.00,  grad_norm=0.207,  \n",
      "Saving model at round 20 and episode 50\n",
      "Round:   20/50  , Iter: 100/250 ,  opt. target:-0.377  mean reward:0.889  H=2.00,  grad_norm=0.251,  \n",
      "Saving model at round 20 and episode 100\n",
      "Round:   20/50  , Iter: 150/250 ,  opt. target:-0.375  mean reward:0.885  H=2.00,  grad_norm=0.291,  \n",
      "Saving model at round 20 and episode 150\n",
      "Round:   20/50  , Iter: 200/250 ,  opt. target:-0.378  mean reward:0.887  H=2.00,  grad_norm=0.549,  \n",
      "Saving model at round 20 and episode 200\n",
      "True test rewards:  0.2115169460386481\n",
      "Round:   21/50  , Iter:   0/250 ,  opt. target:-0.346  mean reward:0.876  H=2.00,  grad_norm=0.272,  \n",
      "Saving model at round 21 and episode 0\n",
      "Round:   21/50  , Iter:  50/250 ,  opt. target:-0.377  mean reward:0.887  H=2.00,  grad_norm=0.082,  \n",
      "Saving model at round 21 and episode 50\n",
      "Round:   21/50  , Iter: 100/250 ,  opt. target:-0.371  mean reward:0.890  H=2.00,  grad_norm=0.047,  \n",
      "Saving model at round 21 and episode 100\n",
      "Round:   21/50  , Iter: 150/250 ,  opt. target:-0.372  mean reward:0.896  H=2.00,  grad_norm=0.293,  \n",
      "Saving model at round 21 and episode 150\n",
      "Round:   21/50  , Iter: 200/250 ,  opt. target:-0.370  mean reward:0.899  H=2.00,  grad_norm=0.330,  \n",
      "Saving model at round 21 and episode 200\n",
      "True test rewards:  0.0040634719429558635\n",
      "Round:   22/50  , Iter:   0/250 ,  opt. target:-0.387  mean reward:0.880  H=2.00,  grad_norm=0.152,  \n",
      "Saving model at round 22 and episode 0\n",
      "Round:   22/50  , Iter:  50/250 ,  opt. target:-0.369  mean reward:0.893  H=2.00,  grad_norm=0.257,  \n",
      "Saving model at round 22 and episode 50\n",
      "Round:   22/50  , Iter: 100/250 ,  opt. target:-0.378  mean reward:0.891  H=2.00,  grad_norm=0.129,  \n",
      "Saving model at round 22 and episode 100\n",
      "Round:   22/50  , Iter: 150/250 ,  opt. target:-0.375  mean reward:0.886  H=2.00,  grad_norm=0.370,  \n",
      "Saving model at round 22 and episode 150\n",
      "Round:   22/50  , Iter: 200/250 ,  opt. target:-0.374  mean reward:0.891  H=2.00,  grad_norm=0.139,  \n",
      "Saving model at round 22 and episode 200\n",
      "True test rewards:  0.012745364199260985\n",
      "Round:   23/50  , Iter:   0/250 ,  opt. target:-0.402  mean reward:0.912  H=2.00,  grad_norm=0.405,  \n",
      "Saving model at round 23 and episode 0\n",
      "Round:   23/50  , Iter:  50/250 ,  opt. target:-0.365  mean reward:0.880  H=2.00,  grad_norm=0.126,  \n",
      "Saving model at round 23 and episode 50\n",
      "Round:   23/50  , Iter: 100/250 ,  opt. target:-0.364  mean reward:0.885  H=2.00,  grad_norm=0.162,  \n",
      "Saving model at round 23 and episode 100\n",
      "Round:   23/50  , Iter: 150/250 ,  opt. target:-0.371  mean reward:0.881  H=2.00,  grad_norm=0.353,  \n",
      "Saving model at round 23 and episode 150\n",
      "Round:   23/50  , Iter: 200/250 ,  opt. target:-0.371  mean reward:0.881  H=2.00,  grad_norm=0.480,  \n",
      "Saving model at round 23 and episode 200\n",
      "True test rewards:  0.01718159625346453\n",
      "Round:   24/50  , Iter:   0/250 ,  opt. target:-0.372  mean reward:0.807  H=2.00,  grad_norm=0.385,  \n",
      "Saving model at round 24 and episode 0\n",
      "Round:   24/50  , Iter:  50/250 ,  opt. target:-0.364  mean reward:0.884  H=2.00,  grad_norm=0.647,  \n",
      "Saving model at round 24 and episode 50\n",
      "Round:   24/50  , Iter: 100/250 ,  opt. target:-0.368  mean reward:0.892  H=2.00,  grad_norm=0.317,  \n",
      "Saving model at round 24 and episode 100\n",
      "Round:   24/50  , Iter: 150/250 ,  opt. target:-0.369  mean reward:0.893  H=2.00,  grad_norm=0.030,  \n",
      "Saving model at round 24 and episode 150\n",
      "Round:   24/50  , Iter: 200/250 ,  opt. target:-0.367  mean reward:0.892  H=2.00,  grad_norm=0.366,  \n",
      "Saving model at round 24 and episode 200\n",
      "True test rewards:  0.2013211348903091\n",
      "Round:   25/50  , Iter:   0/250 ,  opt. target:-0.338  mean reward:0.890  H=2.00,  grad_norm=0.093,  \n",
      "Saving model at round 25 and episode 0\n",
      "Round:   25/50  , Iter:  50/250 ,  opt. target:-0.353  mean reward:0.874  H=2.00,  grad_norm=0.243,  \n",
      "Saving model at round 25 and episode 50\n",
      "Round:   25/50  , Iter: 100/250 ,  opt. target:-0.356  mean reward:0.888  H=2.00,  grad_norm=0.099,  \n",
      "Saving model at round 25 and episode 100\n",
      "Round:   25/50  , Iter: 150/250 ,  opt. target:-0.361  mean reward:0.883  H=2.00,  grad_norm=0.740,  \n",
      "Saving model at round 25 and episode 150\n",
      "Round:   25/50  , Iter: 200/250 ,  opt. target:-0.362  mean reward:0.887  H=2.00,  grad_norm=0.182,  \n",
      "Saving model at round 25 and episode 200\n",
      "True test rewards:  0.13317726578407385\n",
      "Round:   26/50  , Iter:   0/250 ,  opt. target:-0.376  mean reward:0.906  H=2.00,  grad_norm=0.287,  \n",
      "Saving model at round 26 and episode 0\n",
      "Round:   26/50  , Iter:  50/250 ,  opt. target:-0.368  mean reward:0.910  H=2.00,  grad_norm=0.435,  \n",
      "Saving model at round 26 and episode 50\n",
      "Round:   26/50  , Iter: 100/250 ,  opt. target:-0.372  mean reward:0.908  H=2.00,  grad_norm=0.141,  \n",
      "Saving model at round 26 and episode 100\n",
      "Round:   26/50  , Iter: 150/250 ,  opt. target:-0.367  mean reward:0.902  H=2.00,  grad_norm=0.252,  \n",
      "Saving model at round 26 and episode 150\n",
      "Round:   26/50  , Iter: 200/250 ,  opt. target:-0.371  mean reward:0.907  H=2.00,  grad_norm=0.029,  \n",
      "Saving model at round 26 and episode 200\n",
      "True test rewards:  0.09921698168921467\n",
      "Round:   27/50  , Iter:   0/250 ,  opt. target:-0.383  mean reward:0.875  H=2.00,  grad_norm=0.157,  \n",
      "Saving model at round 27 and episode 0\n",
      "Round:   27/50  , Iter:  50/250 ,  opt. target:-0.352  mean reward:0.869  H=2.00,  grad_norm=0.241,  \n",
      "Saving model at round 27 and episode 50\n",
      "Round:   27/50  , Iter: 100/250 ,  opt. target:-0.344  mean reward:0.874  H=2.00,  grad_norm=0.020,  \n",
      "Saving model at round 27 and episode 100\n",
      "Round:   27/50  , Iter: 150/250 ,  opt. target:-0.351  mean reward:0.885  H=2.00,  grad_norm=0.070,  \n",
      "Saving model at round 27 and episode 150\n",
      "Round:   27/50  , Iter: 200/250 ,  opt. target:-0.354  mean reward:0.889  H=2.00,  grad_norm=0.235,  \n",
      "Saving model at round 27 and episode 200\n",
      "True test rewards:  0.17226361998305895\n",
      "Round:   28/50  , Iter:   0/250 ,  opt. target:-0.360  mean reward:0.890  H=2.00,  grad_norm=0.268,  \n",
      "Saving model at round 28 and episode 0\n",
      "Round:   28/50  , Iter:  50/250 ,  opt. target:-0.347  mean reward:0.866  H=2.00,  grad_norm=0.447,  \n",
      "Saving model at round 28 and episode 50\n",
      "Round:   28/50  , Iter: 100/250 ,  opt. target:-0.350  mean reward:0.868  H=2.00,  grad_norm=0.462,  \n",
      "Saving model at round 28 and episode 100\n",
      "Round:   28/50  , Iter: 150/250 ,  opt. target:-0.350  mean reward:0.866  H=2.00,  grad_norm=0.307,  \n",
      "Saving model at round 28 and episode 150\n",
      "Round:   28/50  , Iter: 200/250 ,  opt. target:-0.349  mean reward:0.862  H=2.00,  grad_norm=0.028,  \n",
      "Saving model at round 28 and episode 200\n",
      "True test rewards:  0.11729249236790115\n",
      "Round:   29/50  , Iter:   0/250 ,  opt. target:-0.358  mean reward:0.873  H=2.00,  grad_norm=0.068,  \n",
      "Saving model at round 29 and episode 0\n",
      "Round:   29/50  , Iter:  50/250 ,  opt. target:-0.350  mean reward:0.864  H=2.00,  grad_norm=0.230,  \n",
      "Saving model at round 29 and episode 50\n",
      "Round:   29/50  , Iter: 100/250 ,  opt. target:-0.348  mean reward:0.877  H=2.00,  grad_norm=0.170,  \n",
      "Saving model at round 29 and episode 100\n",
      "Round:   29/50  , Iter: 150/250 ,  opt. target:-0.351  mean reward:0.872  H=2.00,  grad_norm=0.031,  \n",
      "Saving model at round 29 and episode 150\n",
      "Round:   29/50  , Iter: 200/250 ,  opt. target:-0.352  mean reward:0.875  H=2.00,  grad_norm=0.228,  \n",
      "Saving model at round 29 and episode 200\n",
      "True test rewards:  0.13688664361243602\n",
      "Round:   30/50  , Iter:   0/250 ,  opt. target:-0.354  mean reward:0.835  H=2.00,  grad_norm=0.108,  \n",
      "Saving model at round 30 and episode 0\n",
      "Round:   30/50  , Iter:  50/250 ,  opt. target:-0.354  mean reward:0.883  H=2.00,  grad_norm=0.202,  \n",
      "Saving model at round 30 and episode 50\n",
      "Round:   30/50  , Iter: 100/250 ,  opt. target:-0.360  mean reward:0.885  H=2.00,  grad_norm=0.440,  \n",
      "Saving model at round 30 and episode 100\n",
      "Round:   30/50  , Iter: 150/250 ,  opt. target:-0.362  mean reward:0.886  H=2.00,  grad_norm=0.280,  \n",
      "Saving model at round 30 and episode 150\n",
      "Round:   30/50  , Iter: 200/250 ,  opt. target:-0.362  mean reward:0.886  H=2.00,  grad_norm=0.527,  \n",
      "Saving model at round 30 and episode 200\n",
      "True test rewards:  0.11364904533969687\n",
      "Round:   31/50  , Iter:   0/250 ,  opt. target:-0.378  mean reward:0.885  H=2.00,  grad_norm=0.176,  \n",
      "Saving model at round 31 and episode 0\n",
      "Round:   31/50  , Iter:  50/250 ,  opt. target:-0.362  mean reward:0.892  H=2.00,  grad_norm=0.469,  \n",
      "Saving model at round 31 and episode 50\n",
      "Round:   31/50  , Iter: 100/250 ,  opt. target:-0.369  mean reward:0.895  H=2.00,  grad_norm=0.404,  \n",
      "Saving model at round 31 and episode 100\n",
      "Round:   31/50  , Iter: 150/250 ,  opt. target:-0.370  mean reward:0.893  H=2.00,  grad_norm=0.452,  \n",
      "Saving model at round 31 and episode 150\n",
      "Round:   31/50  , Iter: 200/250 ,  opt. target:-0.372  mean reward:0.890  H=2.00,  grad_norm=0.156,  \n",
      "Saving model at round 31 and episode 200\n",
      "True test rewards:  0.12227497414942948\n",
      "Round:   32/50  , Iter:   0/250 ,  opt. target:-0.360  mean reward:0.875  H=2.00,  grad_norm=0.115,  \n",
      "Saving model at round 32 and episode 0\n",
      "Round:   32/50  , Iter:  50/250 ,  opt. target:-0.372  mean reward:0.886  H=2.00,  grad_norm=0.096,  \n",
      "Saving model at round 32 and episode 50\n",
      "Round:   32/50  , Iter: 100/250 ,  opt. target:-0.368  mean reward:0.882  H=2.00,  grad_norm=0.717,  \n",
      "Saving model at round 32 and episode 100\n",
      "Round:   32/50  , Iter: 150/250 ,  opt. target:-0.369  mean reward:0.882  H=2.00,  grad_norm=0.356,  \n",
      "Saving model at round 32 and episode 150\n",
      "Round:   32/50  , Iter: 200/250 ,  opt. target:-0.367  mean reward:0.882  H=2.00,  grad_norm=0.620,  \n",
      "Saving model at round 32 and episode 200\n",
      "True test rewards:  0.06095123827006976\n",
      "Round:   33/50  , Iter:   0/250 ,  opt. target:-0.378  mean reward:0.890  H=2.00,  grad_norm=0.205,  \n",
      "Saving model at round 33 and episode 0\n",
      "Round:   33/50  , Iter:  50/250 ,  opt. target:-0.379  mean reward:0.889  H=2.00,  grad_norm=1.068,  \n",
      "Saving model at round 33 and episode 50\n",
      "Round:   33/50  , Iter: 100/250 ,  opt. target:-0.381  mean reward:0.887  H=2.00,  grad_norm=0.055,  \n",
      "Saving model at round 33 and episode 100\n",
      "Round:   33/50  , Iter: 150/250 ,  opt. target:-0.379  mean reward:0.889  H=2.00,  grad_norm=0.534,  \n",
      "Saving model at round 33 and episode 150\n",
      "Round:   33/50  , Iter: 200/250 ,  opt. target:-0.379  mean reward:0.886  H=2.00,  grad_norm=0.094,  \n",
      "Saving model at round 33 and episode 200\n",
      "True test rewards:  0.050543511909197224\n",
      "Round:   34/50  , Iter:   0/250 ,  opt. target:-0.381  mean reward:0.891  H=2.00,  grad_norm=0.288,  \n",
      "Saving model at round 34 and episode 0\n",
      "Round:   34/50  , Iter:  50/250 ,  opt. target:-0.377  mean reward:0.872  H=2.00,  grad_norm=0.668,  \n",
      "Saving model at round 34 and episode 50\n",
      "Round:   34/50  , Iter: 100/250 ,  opt. target:-0.374  mean reward:0.866  H=2.00,  grad_norm=0.141,  \n",
      "Saving model at round 34 and episode 100\n",
      "Round:   34/50  , Iter: 150/250 ,  opt. target:-0.371  mean reward:0.866  H=2.00,  grad_norm=1.088,  \n",
      "Saving model at round 34 and episode 150\n",
      "Round:   34/50  , Iter: 200/250 ,  opt. target:-0.370  mean reward:0.865  H=2.00,  grad_norm=0.352,  \n",
      "Saving model at round 34 and episode 200\n",
      "True test rewards:  0.13804543025940483\n",
      "Round:   35/50  , Iter:   0/250 ,  opt. target:-0.385  mean reward:0.891  H=2.00,  grad_norm=0.107,  \n",
      "Saving model at round 35 and episode 0\n",
      "Round:   35/50  , Iter:  50/250 ,  opt. target:-0.387  mean reward:0.860  H=2.00,  grad_norm=0.594,  \n",
      "Saving model at round 35 and episode 50\n",
      "Round:   35/50  , Iter: 100/250 ,  opt. target:-0.386  mean reward:0.864  H=2.00,  grad_norm=0.672,  \n",
      "Saving model at round 35 and episode 100\n",
      "Round:   35/50  , Iter: 150/250 ,  opt. target:-0.384  mean reward:0.864  H=2.00,  grad_norm=0.612,  \n",
      "Saving model at round 35 and episode 150\n",
      "Round:   35/50  , Iter: 200/250 ,  opt. target:-0.383  mean reward:0.864  H=2.00,  grad_norm=0.538,  \n",
      "Saving model at round 35 and episode 200\n",
      "True test rewards:  0.12858545305686203\n",
      "Round:   36/50  , Iter:   0/250 ,  opt. target:-0.397  mean reward:0.847  H=2.00,  grad_norm=0.333,  \n",
      "Saving model at round 36 and episode 0\n",
      "Round:   36/50  , Iter:  50/250 ,  opt. target:-0.385  mean reward:0.852  H=2.00,  grad_norm=0.134,  \n",
      "Saving model at round 36 and episode 50\n",
      "Round:   36/50  , Iter: 100/250 ,  opt. target:-0.387  mean reward:0.859  H=2.00,  grad_norm=0.546,  \n",
      "Saving model at round 36 and episode 100\n",
      "Round:   36/50  , Iter: 150/250 ,  opt. target:-0.384  mean reward:0.854  H=2.00,  grad_norm=0.830,  \n",
      "Saving model at round 36 and episode 150\n",
      "Round:   36/50  , Iter: 200/250 ,  opt. target:-0.383  mean reward:0.852  H=2.00,  grad_norm=1.311,  \n",
      "Saving model at round 36 and episode 200\n",
      "True test rewards:  0.1435013087372059\n",
      "Round:   37/50  , Iter:   0/250 ,  opt. target:-0.389  mean reward:0.870  H=2.00,  grad_norm=0.597,  \n",
      "Saving model at round 37 and episode 0\n",
      "Round:   37/50  , Iter:  50/250 ,  opt. target:-0.402  mean reward:0.840  H=2.00,  grad_norm=1.056,  \n",
      "Saving model at round 37 and episode 50\n",
      "Round:   37/50  , Iter: 100/250 ,  opt. target:-0.398  mean reward:0.844  H=2.00,  grad_norm=0.579,  \n",
      "Saving model at round 37 and episode 100\n",
      "Round:   37/50  , Iter: 150/250 ,  opt. target:-0.398  mean reward:0.847  H=2.00,  grad_norm=1.099,  \n",
      "Saving model at round 37 and episode 150\n",
      "Round:   37/50  , Iter: 200/250 ,  opt. target:-0.395  mean reward:0.850  H=2.00,  grad_norm=0.360,  \n",
      "Saving model at round 37 and episode 200\n",
      "True test rewards:  0.20284624895080855\n",
      "Round:   38/50  , Iter:   0/250 ,  opt. target:-0.394  mean reward:0.865  H=2.00,  grad_norm=0.071,  \n",
      "Saving model at round 38 and episode 0\n",
      "Round:   38/50  , Iter:  50/250 ,  opt. target:-0.397  mean reward:0.863  H=2.00,  grad_norm=1.160,  \n",
      "Saving model at round 38 and episode 50\n",
      "Round:   38/50  , Iter: 100/250 ,  opt. target:-0.397  mean reward:0.861  H=2.00,  grad_norm=0.097,  \n",
      "Saving model at round 38 and episode 100\n",
      "Round:   38/50  , Iter: 150/250 ,  opt. target:-0.405  mean reward:0.861  H=2.00,  grad_norm=0.344,  \n",
      "Saving model at round 38 and episode 150\n",
      "Round:   38/50  , Iter: 200/250 ,  opt. target:-0.419  mean reward:0.861  H=2.00,  grad_norm=0.054,  \n",
      "Saving model at round 38 and episode 200\n",
      "True test rewards:  0.23584059408970506\n",
      "Round:   39/50  , Iter:   0/250 ,  opt. target:-0.499  mean reward:0.880  H=2.00,  grad_norm=0.520,  \n",
      "Saving model at round 39 and episode 0\n",
      "Round:   39/50  , Iter:  50/250 ,  opt. target:-0.429  mean reward:0.769  H=2.00,  grad_norm=0.046,  \n",
      "Saving model at round 39 and episode 50\n",
      "Round:   39/50  , Iter: 100/250 ,  opt. target:-0.406  mean reward:0.770  H=2.00,  grad_norm=0.094,  \n",
      "Saving model at round 39 and episode 100\n",
      "Round:   39/50  , Iter: 150/250 ,  opt. target:-0.406  mean reward:0.780  H=2.00,  grad_norm=0.309,  \n",
      "Saving model at round 39 and episode 150\n",
      "Round:   39/50  , Iter: 200/250 ,  opt. target:-0.410  mean reward:0.790  H=2.00,  grad_norm=0.681,  \n",
      "Saving model at round 39 and episode 200\n",
      "True test rewards:  0.1662987796859486\n",
      "Round:   40/50  , Iter:   0/250 ,  opt. target:-0.413  mean reward:0.826  H=2.00,  grad_norm=0.116,  \n",
      "Saving model at round 40 and episode 0\n",
      "Round:   40/50  , Iter:  50/250 ,  opt. target:-0.410  mean reward:0.829  H=2.00,  grad_norm=0.662,  \n",
      "Saving model at round 40 and episode 50\n",
      "Round:   40/50  , Iter: 100/250 ,  opt. target:-0.409  mean reward:0.826  H=2.00,  grad_norm=0.061,  \n",
      "Saving model at round 40 and episode 100\n",
      "Round:   40/50  , Iter: 150/250 ,  opt. target:-0.408  mean reward:0.823  H=2.00,  grad_norm=0.460,  \n",
      "Saving model at round 40 and episode 150\n",
      "Round:   40/50  , Iter: 200/250 ,  opt. target:-0.409  mean reward:0.823  H=2.00,  grad_norm=1.597,  \n",
      "Saving model at round 40 and episode 200\n",
      "True test rewards:  0.16693271775393573\n",
      "Round:   41/50  , Iter:   0/250 ,  opt. target:-0.423  mean reward:0.758  H=2.00,  grad_norm=2.004,  \n",
      "Saving model at round 41 and episode 0\n",
      "Round:   41/50  , Iter:  50/250 ,  opt. target:-0.394  mean reward:0.804  H=2.00,  grad_norm=1.336,  \n",
      "Saving model at round 41 and episode 50\n",
      "Round:   41/50  , Iter: 100/250 ,  opt. target:-0.389  mean reward:0.806  H=2.00,  grad_norm=0.469,  \n",
      "Saving model at round 41 and episode 100\n",
      "Round:   41/50  , Iter: 150/250 ,  opt. target:-0.399  mean reward:0.818  H=2.00,  grad_norm=0.673,  \n",
      "Saving model at round 41 and episode 150\n",
      "Round:   41/50  , Iter: 200/250 ,  opt. target:-0.406  mean reward:0.825  H=2.00,  grad_norm=0.629,  \n",
      "Saving model at round 41 and episode 200\n",
      "True test rewards:  0.17199460638743208\n",
      "Round:   42/50  , Iter:   0/250 ,  opt. target:-0.445  mean reward:0.873  H=2.00,  grad_norm=0.707,  \n",
      "Saving model at round 42 and episode 0\n",
      "Round:   42/50  , Iter:  50/250 ,  opt. target:-0.459  mean reward:0.857  H=2.00,  grad_norm=0.177,  \n",
      "Saving model at round 42 and episode 50\n",
      "Round:   42/50  , Iter: 100/250 ,  opt. target:-0.474  mean reward:0.857  H=2.00,  grad_norm=1.162,  \n",
      "Saving model at round 42 and episode 100\n",
      "Round:   42/50  , Iter: 150/250 ,  opt. target:-0.473  mean reward:0.857  H=2.00,  grad_norm=0.840,  \n",
      "Saving model at round 42 and episode 150\n",
      "Round:   42/50  , Iter: 200/250 ,  opt. target:-0.468  mean reward:0.859  H=2.00,  grad_norm=0.975,  \n",
      "Saving model at round 42 and episode 200\n",
      "True test rewards:  0.17552926929151483\n",
      "Round:   43/50  , Iter:   0/250 ,  opt. target:-0.449  mean reward:0.864  H=2.00,  grad_norm=0.298,  \n",
      "Saving model at round 43 and episode 0\n",
      "Round:   43/50  , Iter:  50/250 ,  opt. target:-0.451  mean reward:0.863  H=2.00,  grad_norm=0.138,  \n",
      "Saving model at round 43 and episode 50\n",
      "Round:   43/50  , Iter: 100/250 ,  opt. target:-0.451  mean reward:0.860  H=2.00,  grad_norm=1.541,  \n",
      "Saving model at round 43 and episode 100\n",
      "Round:   43/50  , Iter: 150/250 ,  opt. target:-0.453  mean reward:0.860  H=2.00,  grad_norm=0.279,  \n",
      "Saving model at round 43 and episode 150\n",
      "Round:   43/50  , Iter: 200/250 ,  opt. target:-0.454  mean reward:0.859  H=2.00,  grad_norm=0.194,  \n",
      "Saving model at round 43 and episode 200\n",
      "True test rewards:  0.17472456007592085\n",
      "Round:   44/50  , Iter:   0/250 ,  opt. target:-0.452  mean reward:0.898  H=2.00,  grad_norm=1.800,  \n",
      "Saving model at round 44 and episode 0\n",
      "Round:   44/50  , Iter:  50/250 ,  opt. target:-0.470  mean reward:0.869  H=2.00,  grad_norm=0.029,  \n",
      "Saving model at round 44 and episode 50\n",
      "Round:   44/50  , Iter: 100/250 ,  opt. target:-0.487  mean reward:0.847  H=2.00,  grad_norm=0.711,  \n",
      "Saving model at round 44 and episode 100\n",
      "Round:   44/50  , Iter: 150/250 ,  opt. target:-0.487  mean reward:0.833  H=2.00,  grad_norm=0.422,  \n",
      "Saving model at round 44 and episode 150\n",
      "Round:   44/50  , Iter: 200/250 ,  opt. target:-0.478  mean reward:0.829  H=2.00,  grad_norm=1.165,  \n",
      "Saving model at round 44 and episode 200\n",
      "True test rewards:  0.2295422424918519\n",
      "Round:   45/50  , Iter:   0/250 ,  opt. target:-0.460  mean reward:0.800  H=2.00,  grad_norm=0.966,  \n",
      "Saving model at round 45 and episode 0\n",
      "Round:   45/50  , Iter:  50/250 ,  opt. target:-0.464  mean reward:0.845  H=2.00,  grad_norm=0.865,  \n",
      "Saving model at round 45 and episode 50\n",
      "Round:   45/50  , Iter: 100/250 ,  opt. target:-0.465  mean reward:0.840  H=2.00,  grad_norm=0.232,  \n",
      "Saving model at round 45 and episode 100\n",
      "Round:   45/50  , Iter: 150/250 ,  opt. target:-0.470  mean reward:0.841  H=2.00,  grad_norm=0.513,  \n",
      "Saving model at round 45 and episode 150\n",
      "Round:   45/50  , Iter: 200/250 ,  opt. target:-0.474  mean reward:0.845  H=2.00,  grad_norm=0.538,  \n",
      "Saving model at round 45 and episode 200\n",
      "True test rewards:  0.25228229562720117\n",
      "Round:   46/50  , Iter:   0/250 ,  opt. target:-0.465  mean reward:0.857  H=2.00,  grad_norm=0.670,  \n",
      "Saving model at round 46 and episode 0\n",
      "Round:   46/50  , Iter:  50/250 ,  opt. target:-0.460  mean reward:0.828  H=2.00,  grad_norm=1.043,  \n",
      "Saving model at round 46 and episode 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 2.1 seconds.), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 4.2 seconds.), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 9.1 seconds.), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 17.1 seconds.), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 32.1 seconds.), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 64.9 seconds.), retrying request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:   46/50  , Iter: 100/250 ,  opt. target:-0.462  mean reward:0.819  H=2.00,  grad_norm=1.646,  \n",
      "Saving model at round 46 and episode 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 134.9 seconds.), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 293.4 seconds.), retrying request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:   46/50  , Iter: 150/250 ,  opt. target:-0.463  mean reward:0.824  H=2.00,  grad_norm=0.304,  \n",
      "Saving model at round 46 and episode 150\n",
      "Round:   46/50  , Iter: 200/250 ,  opt. target:-0.464  mean reward:0.826  H=2.00,  grad_norm=0.229,  \n",
      "Saving model at round 46 and episode 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 368.6 seconds.), retrying request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True test rewards:  0.2513452388659245\n",
      "Round:   47/50  , Iter:   0/250 ,  opt. target:-0.474  mean reward:0.835  H=2.00,  grad_norm=0.740,  \n",
      "Saving model at round 47 and episode 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 346.8 seconds.), retrying request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:   47/50  , Iter:  50/250 ,  opt. target:-0.490  mean reward:0.868  H=2.00,  grad_norm=1.361,  \n",
      "Saving model at round 47 and episode 50\n",
      "Round:   47/50  , Iter: 100/250 ,  opt. target:-0.486  mean reward:0.877  H=2.00,  grad_norm=0.234,  \n",
      "Saving model at round 47 and episode 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 373.6 seconds.), retrying request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:   47/50  , Iter: 150/250 ,  opt. target:-0.476  mean reward:0.870  H=2.00,  grad_norm=1.104,  \n",
      "Saving model at round 47 and episode 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 363.8 seconds.), retrying request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:   47/50  , Iter: 200/250 ,  opt. target:-0.472  mean reward:0.871  H=2.00,  grad_norm=0.255,  \n",
      "Saving model at round 47 and episode 200\n",
      "True test rewards:  0.25208232945612263\n",
      "Round:   48/50  , Iter:   0/250 ,  opt. target:-0.439  mean reward:0.877  H=2.00,  grad_norm=0.611,  \n",
      "Saving model at round 48 and episode 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 354.7 seconds.), retrying request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:   48/50  , Iter:  50/250 ,  opt. target:-0.468  mean reward:0.881  H=2.00,  grad_norm=1.004,  \n",
      "Saving model at round 48 and episode 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 360.1 seconds.), retrying request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:   48/50  , Iter: 100/250 ,  opt. target:-0.483  mean reward:0.874  H=2.00,  grad_norm=0.752,  \n",
      "Saving model at round 48 and episode 100\n",
      "Round:   48/50  , Iter: 150/250 ,  opt. target:-0.480  mean reward:0.863  H=2.00,  grad_norm=0.278,  \n",
      "Saving model at round 48 and episode 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 306.5 seconds.), retrying request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:   48/50  , Iter: 200/250 ,  opt. target:-0.476  mean reward:0.860  H=2.00,  grad_norm=0.602,  \n",
      "Saving model at round 48 and episode 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 328.5 seconds.), retrying request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True test rewards:  0.29392387881659626\n",
      "Round:   49/50  , Iter:   0/250 ,  opt. target:-0.440  mean reward:0.869  H=2.00,  grad_norm=0.148,  \n",
      "Saving model at round 49 and episode 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 310.2 seconds.), retrying request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:   49/50  , Iter:  50/250 ,  opt. target:-0.467  mean reward:0.883  H=2.00,  grad_norm=0.761,  \n",
      "Saving model at round 49 and episode 50\n",
      "Round:   49/50  , Iter: 100/250 ,  opt. target:-0.470  mean reward:0.873  H=2.00,  grad_norm=0.578,  \n",
      "Saving model at round 49 and episode 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 341.5 seconds.), retrying request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:   49/50  , Iter: 150/250 ,  opt. target:-0.468  mean reward:0.882  H=2.00,  grad_norm=0.390,  \n",
      "Saving model at round 49 and episode 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 328.4 seconds.), retrying request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:   49/50  , Iter: 200/250 ,  opt. target:-0.465  mean reward:0.886  H=2.00,  grad_norm=0.640,  \n",
      "Saving model at round 49 and episode 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 335.8 seconds.), retrying request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True test rewards:  0.27148873619901603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td></td></tr><tr><td>grad_norm</td><td></td></tr><tr><td>mean reward</td><td></td></tr><tr><td>opt. target</td><td></td></tr><tr><td>round</td><td></td></tr><tr><td>true test rewards</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>249</td></tr><tr><td>grad_norm</td><td>0.47404</td></tr><tr><td>mean reward</td><td>0.88935</td></tr><tr><td>opt. target</td><td>-0.46409</td></tr><tr><td>round</td><td>49</td></tr><tr><td>true test rewards</td><td>0.27149</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">azure-monkey-7</strong> at: <a href='https://wandb.ai/bavci/swingup_ctpole/runs/9dmzdjtu' target=\"_blank\">https://wandb.ai/bavci/swingup_ctpole/runs/9dmzdjtu</a><br/> View project at: <a href='https://wandb.ai/bavci/swingup_ctpole' target=\"_blank\">https://wandb.ai/bavci/swingup_ctpole</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240817_204216-9dmzdjtu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "def discount_rewards(r, gamma):\n",
    "    discounted_r = torch.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size(-1))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "# random seed\n",
    "wandb.init(project=\"swingup_ctpole\", entity=\"bavci\")\n",
    "# Set seed for reproducibility\n",
    "device = env.device\n",
    "\n",
    "# Initialize policy network\n",
    "policy_nn = Policy(env)\n",
    "policy_nn.to(device)\n",
    "\n",
    "# Initialize value network\n",
    "us_V = basic_mdl(env.n, 1, n_hid_layers=2, act=\"tanh\", n_hidden=200)\n",
    "us_V.reset_parameters()\n",
    "us_V.to(device)\n",
    "\n",
    "# Initialize optimizers\n",
    "policy_opt = optim.Adam(policy_nn.parameters(), lr=0.001)\n",
    "us_V_opt = optim.Adam(us_V.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_episodes =250\n",
    "num_steps = 20\n",
    "num_rounds = 50\n",
    "gamma = 0.99\n",
    "tau = 2.0\n",
    "\n",
    "\n",
    "flag = True\n",
    "\n",
    "for rounds in range(num_rounds):\n",
    "\n",
    "    # if mean_reward > 0.2 decreased the learning rate\n",
    "\n",
    "\n",
    "    rewards,opt_objs = [],[]\n",
    "    for episode in range(num_episodes):\n",
    "        if episode%50==0:\n",
    "            Vtarget = copy.deepcopy(us_V)\n",
    "\n",
    "\n",
    "        initial_observations = [env.reset() for _ in range(50)]\n",
    "        s0 = torch.stack([env.obs2state(torch.tensor(obs, device=device)) for obs in initial_observations])\n",
    "        ts = env.build_time_grid(num_steps).to(device)\n",
    "        policy_opt.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "        st, at, rt, ts  = env.integrate_system(T=num_steps, g=policy_nn, s0=s0, N=1)\n",
    "        # print(rt.shape)\n",
    "        rew_int  = rt[:,-1].mean(0)  # N\n",
    "        # print(rew_int.shape)\n",
    "    \n",
    "        # print(st.shape)\n",
    "        st = torch.cat([st]*5) if st.shape[0]==1 else st\n",
    "        # print(st.shape)\n",
    "        ts = ts[0]\n",
    "        gammas = (-ts/tau).exp() # H\n",
    "        # print(us_V(st.contiguous()).shape)\n",
    "        V_st_gam = us_V(st.contiguous())[:,1:,0] * gammas[1:] # L,N,H-1\n",
    "        # print(V_st_gam.shape)\n",
    "        V_const = min(rounds/5.0,1)\n",
    "        # print((V_const*V_st_gam).shape)\n",
    "        # print(rt[:,1:].shape)\n",
    "        n_step_returns = rt[:,1:] + V_const*V_st_gam # ---> n_step_returns[:,:,k] is the sum in (5)\n",
    "        # print(\"nstep\",n_step_returns.shape)\n",
    "        optimized_returns = n_step_returns.mean(-1) # L,N\n",
    "        # print(\"optimized\",optimized_returns.shape)\n",
    "        # print(optimized_returns)\n",
    "        mean_cost = -optimized_returns.mean()\n",
    "        # print(mean_cost.shape)\n",
    "        # print(mean_cost)\n",
    "        mean_cost.backward()\n",
    "        grad_norm = torch.norm(flatten_([p.grad for p in policy_nn.parameters()])).item()\n",
    "        policy_opt.step()\n",
    "\n",
    "        rewards.append(rew_int.mean().item())\n",
    "        opt_objs.append(mean_cost.mean().item())\n",
    "        print_log = 'Round: {:4d}/{:<4d}, Iter:{:4d}/{:<4d},  opt. target:{:.3f}  mean reward:{:.3f}  '\\\n",
    "                .format(rounds,num_rounds, episode, num_episodes, np.mean(opt_objs), np.mean(rewards)) + \\\n",
    "                'H={:.2f},  grad_norm={:.3f},  '.format(2.0,grad_norm)\n",
    "        wandb.log({\"round\":rounds, \"episode\":episode, \"opt. target\":np.mean(opt_objs), \"mean reward\":np.mean(rewards), \"grad_norm\":grad_norm})\n",
    "        # if rounds > 0:\n",
    "        #     if np.mean(rewards) > 0.15 and flag:\n",
    "        #         policy_opt = optim.Adam(policy_nn.parameters(), lr=0.001)\n",
    "        #         us_V_opt = optim.Adam(us_V.parameters(), lr=0.001)\n",
    "        #         flag = False\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # regress all intermediate values\n",
    "            # print(\"Regressing all intermediate values\")\n",
    "            # print(st.detach().contiguous().shape)\n",
    "            last_states = st.detach().contiguous()[:,1:,:] # L,N,T-1,n\n",
    "            # print(\"last\",last_states.shape)\n",
    "            last_values = Vtarget(last_states).squeeze(-1)\n",
    "            # print(\"last_val\",last_values.shape)\n",
    "            # print(((-ts[1:]/tau).exp()*last_values ).shape)\n",
    "            # print((rt[:,1:,:].squeeze()).shape)\n",
    "            Vtargets = rt[:,1:].squeeze() + (-ts[1:]/tau).exp()*last_values # L,N,T-1\n",
    "            # print(\"Vvv\",Vtargets.shape)\n",
    "            Vtargets = Vtargets.mean(-1)\n",
    "            # print(Vtargets.shape)\n",
    "        mean_val_err = 0\n",
    "\n",
    "\n",
    "        for inner_iter in range(10):\n",
    "            us_V_opt.zero_grad()\n",
    "            # print(\"USV\",us_V(s0).squeeze(-1).shape)\n",
    "            # print(s0.shape)\n",
    "            td_error = us_V(s0).squeeze(-1) - Vtargets # L,N\n",
    "            td_error = torch.mean(td_error**2)\n",
    "            # print(td_error)\n",
    "            td_error.backward()\n",
    "            mean_val_err += td_error.item() / 10\n",
    "            if inner_iter==0:\n",
    "                first_val_err = td_error.item()\n",
    "            us_V_opt.step()\n",
    "\n",
    "        if episode%(num_episodes//5)==0:\n",
    "            print(print_log)\n",
    "            # Save the model indicate rounds and and episode\n",
    "            if not os.path.exists('models/'):\n",
    "                os.makedirs('models/')\n",
    "\n",
    "            if not os.path.exists('models/policy8'):\n",
    "                os.makedirs('models/policy8')\n",
    "            \n",
    "            if not os.path.exists('models/value8'):\n",
    "                os.makedirs('models/value8')\n",
    "\n",
    "            \n",
    "\n",
    "            print(\"Saving model at round {} and episode {}\".format(rounds, episode))\n",
    "            torch.save(policy_nn.state_dict(), 'models/policy8/round_{}_episode_{}.pt'.format(rounds, episode))\n",
    "            torch.save(us_V.state_dict(), 'models/value8/round_{}_episode_{}.pt'.format(rounds, episode))\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Htest,Ntest,Tup = 30,10,int(3.0/0.1)\n",
    "        initial_observations = [env.reset() for _ in range(10)]\n",
    "        s0 = torch.stack([env.obs2state(torch.tensor(obs, device=device)) for obs in initial_observations])\n",
    "        test_states, test_actions, test_rewards,_ = env.integrate_system(T=200, s0=s0, g=policy_nn)\n",
    "        \n",
    "        true_test_rewards = test_rewards[...,Tup:].mean().item()\n",
    "        print(\"True test rewards: \", true_test_rewards)\n",
    "        wandb.log({\"true test rewards\":true_test_rewards})\n",
    "        \n",
    "        if true_test_rewards > 0.9:\n",
    "            print(\"Test rewards > 0.9. Training complete...\")\n",
    "\n",
    "            break\n",
    "\n",
    "        env.close()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/13/avcib1/unix/.pyenv/versions/3.7.7/envs/myproject/lib/python3.7/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:    0/50  , Iter:   0/250 ,  opt. target:0.004  mean reward:0.000  H=2.00,  grad_norm=0.010,  \n",
      "Saving model at round 0 and episode 0\n",
      "Round:    0/50  , Iter:  50/250 ,  opt. target:-0.856  mean reward:0.479  H=2.00,  grad_norm=0.319,  \n",
      "Saving model at round 0 and episode 50\n",
      "Round:    0/50  , Iter: 100/250 ,  opt. target:-0.905  mean reward:0.487  H=2.00,  grad_norm=0.977,  \n",
      "Saving model at round 0 and episode 100\n",
      "Round:    0/50  , Iter: 150/250 ,  opt. target:-0.927  mean reward:0.490  H=2.00,  grad_norm=0.218,  \n",
      "Saving model at round 0 and episode 150\n",
      "Round:    0/50  , Iter: 200/250 ,  opt. target:-0.940  mean reward:0.492  H=2.00,  grad_norm=0.231,  \n",
      "Saving model at round 0 and episode 200\n",
      "True test rewards:  0.6188837560979294\n",
      "Round:    1/50  , Iter:   0/250 ,  opt. target:-1.571  mean reward:0.498  H=2.00,  grad_norm=0.920,  \n",
      "Saving model at round 1 and episode 0\n",
      "Round:    1/50  , Iter:  50/250 ,  opt. target:-1.627  mean reward:0.498  H=2.00,  grad_norm=0.217,  \n",
      "Saving model at round 1 and episode 50\n",
      "Round:    1/50  , Iter: 100/250 ,  opt. target:-1.627  mean reward:0.499  H=2.00,  grad_norm=0.445,  \n",
      "Saving model at round 1 and episode 100\n",
      "Round:    1/50  , Iter: 150/250 ,  opt. target:-1.646  mean reward:0.499  H=2.00,  grad_norm=0.329,  \n",
      "Saving model at round 1 and episode 150\n",
      "Round:    1/50  , Iter: 200/250 ,  opt. target:-1.654  mean reward:0.498  H=2.00,  grad_norm=0.134,  \n",
      "Saving model at round 1 and episode 200\n",
      "True test rewards:  0.5097415845555954\n",
      "Round:    2/50  , Iter:   0/250 ,  opt. target:-2.472  mean reward:0.500  H=2.00,  grad_norm=0.174,  \n",
      "Saving model at round 2 and episode 0\n",
      "Round:    2/50  , Iter:  50/250 ,  opt. target:-2.529  mean reward:0.499  H=2.00,  grad_norm=0.202,  \n",
      "Saving model at round 2 and episode 50\n",
      "Round:    2/50  , Iter: 100/250 ,  opt. target:-2.532  mean reward:0.499  H=2.00,  grad_norm=0.203,  \n",
      "Saving model at round 2 and episode 100\n",
      "Round:    2/50  , Iter: 150/250 ,  opt. target:-2.552  mean reward:0.499  H=2.00,  grad_norm=0.548,  \n",
      "Saving model at round 2 and episode 150\n",
      "Round:    2/50  , Iter: 200/250 ,  opt. target:-2.562  mean reward:0.499  H=2.00,  grad_norm=0.089,  \n",
      "Saving model at round 2 and episode 200\n",
      "True test rewards:  0.8105369451577553\n",
      "Round:    3/50  , Iter:   0/250 ,  opt. target:-3.460  mean reward:0.500  H=2.00,  grad_norm=0.081,  \n",
      "Saving model at round 3 and episode 0\n",
      "Round:    3/50  , Iter:  50/250 ,  opt. target:-3.519  mean reward:0.500  H=2.00,  grad_norm=0.201,  \n",
      "Saving model at round 3 and episode 50\n",
      "Round:    3/50  , Iter: 100/250 ,  opt. target:-3.519  mean reward:0.500  H=2.00,  grad_norm=0.125,  \n",
      "Saving model at round 3 and episode 100\n",
      "Round:    3/50  , Iter: 150/250 ,  opt. target:-3.538  mean reward:0.500  H=2.00,  grad_norm=0.338,  \n",
      "Saving model at round 3 and episode 150\n",
      "Round:    3/50  , Iter: 200/250 ,  opt. target:-3.545  mean reward:0.500  H=2.00,  grad_norm=0.866,  \n",
      "Saving model at round 3 and episode 200\n",
      "True test rewards:  0.8183024238529165\n",
      "Round:    4/50  , Iter:   0/250 ,  opt. target:-4.483  mean reward:0.500  H=2.00,  grad_norm=0.120,  \n",
      "Saving model at round 4 and episode 0\n",
      "Round:    4/50  , Iter:  50/250 ,  opt. target:-4.525  mean reward:0.500  H=2.00,  grad_norm=0.062,  \n",
      "Saving model at round 4 and episode 50\n",
      "Round:    4/50  , Iter: 100/250 ,  opt. target:-4.526  mean reward:0.500  H=2.00,  grad_norm=0.184,  \n",
      "Saving model at round 4 and episode 100\n",
      "Round:    4/50  , Iter: 150/250 ,  opt. target:-4.537  mean reward:0.500  H=2.00,  grad_norm=0.066,  \n",
      "Saving model at round 4 and episode 150\n",
      "Round:    4/50  , Iter: 200/250 ,  opt. target:-4.543  mean reward:0.500  H=2.00,  grad_norm=0.220,  \n",
      "Saving model at round 4 and episode 200\n",
      "True test rewards:  0.7826388072644328\n",
      "Round:    5/50  , Iter:   0/250 ,  opt. target:-5.496  mean reward:0.500  H=2.00,  grad_norm=0.054,  \n",
      "Saving model at round 5 and episode 0\n",
      "Round:    5/50  , Iter:  50/250 ,  opt. target:-5.521  mean reward:0.500  H=2.00,  grad_norm=0.143,  \n",
      "Saving model at round 5 and episode 50\n",
      "Round:    5/50  , Iter: 100/250 ,  opt. target:-5.515  mean reward:0.497  H=2.00,  grad_norm=4.010,  \n",
      "Saving model at round 5 and episode 100\n",
      "Round:    5/50  , Iter: 150/250 ,  opt. target:-5.510  mean reward:0.489  H=2.00,  grad_norm=0.378,  \n",
      "Saving model at round 5 and episode 150\n",
      "Round:    5/50  , Iter: 200/250 ,  opt. target:-5.499  mean reward:0.483  H=2.00,  grad_norm=1.746,  \n",
      "Saving model at round 5 and episode 200\n",
      "True test rewards:  0.02811712418598592\n",
      "Round:    6/50  , Iter:   0/250 ,  opt. target:-5.506  mean reward:0.497  H=2.00,  grad_norm=1.373,  \n",
      "Saving model at round 6 and episode 0\n",
      "Round:    6/50  , Iter:  50/250 ,  opt. target:-5.482  mean reward:0.480  H=2.00,  grad_norm=1.860,  \n",
      "Saving model at round 6 and episode 50\n",
      "Round:    6/50  , Iter: 100/250 ,  opt. target:-5.494  mean reward:0.486  H=2.00,  grad_norm=1.577,  \n",
      "Saving model at round 6 and episode 100\n",
      "Round:    6/50  , Iter: 150/250 ,  opt. target:-5.501  mean reward:0.482  H=2.00,  grad_norm=2.938,  \n",
      "Saving model at round 6 and episode 150\n",
      "Round:    6/50  , Iter: 200/250 ,  opt. target:-5.517  mean reward:0.484  H=2.00,  grad_norm=0.984,  \n",
      "Saving model at round 6 and episode 200\n",
      "True test rewards:  0.9979141779860211\n",
      "Test rewards > 0.9. Training complete...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def discount_rewards(r, gamma):\n",
    "    discounted_r = torch.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size(-1))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "# random seed\n",
    "\n",
    "# Set seed for reproducibility\n",
    "device = env.device\n",
    "\n",
    "# Initialize policy network\n",
    "policy_nn = Policy(env)\n",
    "policy_nn.to(device)\n",
    "\n",
    "# Initialize value network\n",
    "us_V = basic_mdl(env.n, 1, n_hid_layers=2, act=\"tanh\", n_hidden=200)\n",
    "us_V.reset_parameters()\n",
    "us_V.to(device)\n",
    "\n",
    "\n",
    "rounds = 10\n",
    "episode = 200\n",
    "policy_nn = Policy(env)\n",
    "policy_nn.load_state_dict(torch.load('models/policy10/round_{}_episode_{}.pt'.format(rounds, episode)))\n",
    "policy_nn.to(device)\n",
    "policy_nn.train()\n",
    "\n",
    "us_V = basic_mdl(env.n, 1, n_hid_layers=2, act=\"tanh\", n_hidden=200)\n",
    "us_V.load_state_dict(torch.load('models/value10/round_{}_episode_{}.pt'.format(rounds, episode)))\n",
    "us_V.to(device)\n",
    "us_V.train()\n",
    "# Initialize optimizers\n",
    "policy_opt = optim.Adam(policy_nn.parameters(), lr=0.001)\n",
    "us_V_opt = optim.Adam(us_V.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_episodes =250\n",
    "num_steps = 20\n",
    "num_rounds = 50\n",
    "gamma = 0.99\n",
    "tau = 5.0\n",
    "\n",
    "experience_buffer = []\n",
    "flag = True\n",
    "\n",
    "for rounds in range(num_rounds):\n",
    "\n",
    "    # if mean_reward > 0.2 decreased the learning rate\n",
    "\n",
    "\n",
    "    rewards,opt_objs = [],[]\n",
    "    for episode in range(num_episodes):\n",
    "        if episode%100==0:\n",
    "            Vtarget = copy.deepcopy(us_V)\n",
    "\n",
    "\n",
    "        import random\n",
    "\n",
    "        # Draw initial states from previous experiences\n",
    "        if len(experience_buffer) >= 50:\n",
    "            initial_observations = random.sample(experience_buffer, 50)\n",
    "\n",
    "        else:\n",
    "            # If buffer is empty or not enough experiences, reset the environment\n",
    "            initial_observations = [env.reset() for _ in range(50)]\n",
    "\n",
    "        s0 = torch.stack([env.obs2state(torch.tensor(obs, device=device)) for obs in initial_observations])\n",
    "        ts = env.build_time_grid(num_steps).to(device)\n",
    "        policy_opt.zero_grad()\n",
    "\n",
    "        st, at, rt, ts  = env.integrate_system(T=num_steps, g=policy_nn, s0=s0, N=1)\n",
    "\n",
    "        rew_int  = rt[:,-1].mean(0)  # N\n",
    "        # print(rew_int.shape)\n",
    "        st_contiguous = st.contiguous()\n",
    "\n",
    "        # Reshape the tensor to combine the first two dimensions\n",
    "        st_flattened = st_contiguous.reshape(-1, st.size(-1))  # This results in shape (1000, 4)\n",
    "\n",
    "        # Randomly sample 50 states from the flattened tensor\n",
    "        indices = torch.randperm(st_flattened.size(0))[:50]\n",
    "        random_states = st_flattened[indices]\n",
    "        experience_buffer.extend(random_states.detach().cpu().numpy())\n",
    "        \n",
    "        # print(st.shape)\n",
    "        st = torch.cat([st]*5) if st.shape[0]==1 else st\n",
    "        # print(st.shape)\n",
    "        ts = ts[0]\n",
    "        gammas = (-ts/tau).exp() # H\n",
    "        # print(us_V(st.contiguous()).shape)\n",
    "        V_st_gam = us_V(st.contiguous())[:,1:,0] * gammas[1:] # L,N,H-1\n",
    "        # print(V_st_gam.shape)\n",
    "        V_const = min(rounds/5.0,1)\n",
    "        # print((V_const*V_st_gam).shape)\n",
    "        # print(rt[:,1:].shape)\n",
    "        n_step_returns = rt[:,1:] + V_const*V_st_gam # ---> n_step_returns[:,:,k] is the sum in (5)\n",
    "        # print(\"nstep\",n_step_returns.shape)\n",
    "        optimized_returns = n_step_returns.mean(-1) # L,N\n",
    "        # print(\"optimized\",optimized_returns.shape)\n",
    "        # print(optimized_returns)\n",
    "        mean_cost = -optimized_returns.mean()\n",
    "        # print(mean_cost.shape)\n",
    "        # print(mean_cost)\n",
    "        mean_cost.backward()\n",
    "        grad_norm = torch.norm(flatten_([p.grad for p in policy_nn.parameters()])).item()\n",
    "        policy_opt.step()\n",
    "\n",
    "        rewards.append(rew_int.mean().item()/2.0)\n",
    "        opt_objs.append(mean_cost.mean().item())\n",
    "        print_log = 'Round: {:4d}/{:<4d}, Iter:{:4d}/{:<4d},  opt. target:{:.3f}  mean reward:{:.3f}  '\\\n",
    "                .format(rounds,num_rounds, episode, num_episodes, np.mean(opt_objs), np.mean(rewards)) + \\\n",
    "                'H={:.2f},  grad_norm={:.3f},  '.format(2.0,grad_norm)\n",
    "        \n",
    "        # if rounds > 0:\n",
    "        #     if np.mean(rewards) > 0.15 and flag:\n",
    "        #         policy_opt = optim.Adam(policy_nn.parameters(), lr=0.001)\n",
    "        #         us_V_opt = optim.Adam(us_V.parameters(), lr=0.001)\n",
    "        #         flag = False\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # regress all intermediate values\n",
    "            # print(\"Regressing all intermediate values\")\n",
    "            # print(st.detach().contiguous().shape)\n",
    "            last_states = st.detach().contiguous()[:,1:,:] # L,N,T-1,n\n",
    "            # print(\"last\",last_states.shape)\n",
    "            last_values = Vtarget(last_states).squeeze(-1)\n",
    "            # print(\"last_val\",last_values.shape)\n",
    "            # print(((-ts[1:]/tau).exp()*last_values ).shape)\n",
    "            # print((rt[:,1:,:].squeeze()).shape)\n",
    "            Vtargets = rt[:,1:].squeeze() + (-ts[1:]/tau).exp()*last_values # L,N,T-1\n",
    "            # print(\"Vvv\",Vtargets.shape)\n",
    "            Vtargets = Vtargets.mean(-1)\n",
    "            # print(Vtargets.shape)\n",
    "        mean_val_err = 0\n",
    "\n",
    "\n",
    "        for inner_iter in range(10):\n",
    "            us_V_opt.zero_grad()\n",
    "            # print(\"USV\",us_V(s0).squeeze(-1).shape)\n",
    "            # print(s0.shape)\n",
    "            td_error = us_V(s0).squeeze(-1) - Vtargets # L,N\n",
    "            td_error = torch.mean(td_error**2)\n",
    "            # print(td_error)\n",
    "            td_error.backward()\n",
    "            mean_val_err += td_error.item() / 10\n",
    "            if inner_iter==0:\n",
    "                first_val_err = td_error.item()\n",
    "            us_V_opt.step()\n",
    "\n",
    "        if episode%(num_episodes//5)==0:\n",
    "            print(print_log)\n",
    "            # Save the model indicate rounds and and episode\n",
    "            if not os.path.exists('models/'):\n",
    "                os.makedirs('models/')\n",
    "\n",
    "            if not os.path.exists('models/policy11'):\n",
    "                os.makedirs('models/policy11')\n",
    "            \n",
    "            if not os.path.exists('models/value11'):\n",
    "                os.makedirs('models/value11')\n",
    "\n",
    "            \n",
    "\n",
    "            print(\"Saving model at round {} and episode {}\".format(rounds, episode))\n",
    "            torch.save(policy_nn.state_dict(), 'models/policy11/round_{}_episode_{}.pt'.format(rounds, episode))\n",
    "            torch.save(us_V.state_dict(), 'models/value11/round_{}_episode_{}.pt'.format(rounds, episode))\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Htest,Ntest,Tup = 30,10,int(3.0/0.1)\n",
    "        initial_observations = [env.reset() for _ in range(10)]\n",
    "        s0 = torch.stack([env.obs2state(torch.tensor(obs, device=device)) for obs in initial_observations])\n",
    "        test_states, test_actions, test_rewards,_ = env.integrate_system(T=300, s0=s0, g=policy_nn)\n",
    "        \n",
    "        true_test_rewards = test_rewards[...,Tup:].mean().item()\n",
    "        print(\"True test rewards: \", true_test_rewards)\n",
    "        \n",
    "        if true_test_rewards > 0.9:\n",
    "            print(\"Test rewards > 0.9. Training complete...\")\n",
    "\n",
    "            break\n",
    "\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True test reward: 0.170\n"
     ]
    }
   ],
   "source": [
    "# Run saved policy on the environment\n",
    "import time\n",
    "\n",
    "rounds = 18\n",
    "episode = 200\n",
    "policy_nn = Policy(env)\n",
    "policy_nn.load_state_dict(torch.load('models/policy10/round_{}_episode_{}.pt'.format(rounds, episode)))\n",
    "policy_nn.to(device)\n",
    "policy_nn.eval()\n",
    "\n",
    "us_V = basic_mdl(env.n, 1, n_hid_layers=2, act=\"tanh\", n_hidden=200)\n",
    "us_V.load_state_dict(torch.load('models/value10/round_{}_episode_{}.pt'.format(rounds, episode)))\n",
    "us_V.to(device)\n",
    "us_V.eval()\n",
    "1\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    Htest,Ntest,Tup = 30,10,int(3.0/0.1)\n",
    "    initial_observations = [env.reset() for _ in range(10)]\n",
    "    s0 = torch.stack([env.obs2state(torch.tensor(obs, device=device)) for obs in initial_observations])\n",
    "    test_states,test_actions, test_rewards,_ = env.integrate_system(T=200, s0=s0, g=policy_nn)\n",
    "    # print(test_rewards)\n",
    "    # print(test_actions)\n",
    "    true_test_rewards = test_rewards[...,Tup:].mean().item()\n",
    "    print('True test reward: {:.3f}'.format(true_test_rewards))\n",
    "    for step in range(200):\n",
    "        observation = test_states[0, step].cpu().numpy()\n",
    "        env.set_state_(observation)\n",
    "        time.sleep(0.01)\n",
    "\n",
    "        img = env.render(mode='rgb_array')\n",
    "\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True test reward: -0.001\n",
      "True test reward: 0.000\n",
      "True test reward: -0.000\n",
      "True test reward: -0.005\n",
      "True test reward: -0.005\n",
      "True test reward: -0.003\n",
      "True test reward: -0.005\n",
      "True test reward: 0.015\n",
      "True test reward: 0.012\n",
      "True test reward: 0.022\n",
      "True test reward: 0.021\n",
      "True test reward: 0.041\n",
      "True test reward: 0.035\n",
      "True test reward: 0.047\n",
      "True test reward: 0.063\n",
      "True test reward: 0.066\n",
      "True test reward: 0.047\n",
      "True test reward: 0.043\n",
      "True test reward: 0.049\n",
      "True test reward: 0.035\n",
      "True test reward: 0.057\n",
      "True test reward: 0.105\n",
      "True test reward: 0.120\n",
      "True test reward: 0.444\n",
      "True test reward: 0.588\n",
      "True test reward: 0.710\n",
      "True test reward: 0.526\n",
      "True test reward: 0.629\n",
      "True test reward: 0.607\n",
      "True test reward: 0.759\n",
      "True test reward: 0.772\n",
      "True test reward: 0.491\n",
      "True test reward: 0.736\n",
      "True test reward: 0.037\n",
      "True test reward: 0.029\n",
      "True test reward: 0.995\n"
     ]
    }
   ],
   "source": [
    "# To create a video of evaluation of trained policy run all the saved policies in a for loop and make a video\n",
    "\n",
    "import time\n",
    "import imageio\n",
    "import pickle\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "total_rounds = 18\n",
    "total_episodes = 250\n",
    "\n",
    "# episodes has 0 20 40 60 80\n",
    "\n",
    "\n",
    "for rounds in range(total_rounds):\n",
    "    for episode in range(0, total_episodes,200):\n",
    "\n",
    "        policy_nn = Policy(env)\n",
    "        policy_nn.load_state_dict(torch.load('models/policy10/round_{}_episode_{}.pt'.format(rounds, episode)))\n",
    "        policy_nn.to(device)\n",
    "        policy_nn.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Htest,Ntest,Tup = 30,10,int(3.0/0.1)\n",
    "            initial_observations = [env.reset() for _ in range(10)]\n",
    "            s0 = torch.stack([env.obs2state(torch.tensor(obs, device=device)) for obs in initial_observations])\n",
    "            test_states,test_actions, test_rewards,_ = env.integrate_system(T=300, s0=s0, g=policy_nn)\n",
    "            # print(test_rewards)\n",
    "            # print(test_actions)\n",
    "            true_test_rewards = test_rewards[...,Tup:].mean().item()\n",
    "            print('True test reward: {:.3f}'.format(true_test_rewards))\n",
    "            #save test states, actions and rewards pkl\n",
    "            with open('test_states_actions_rewards_acrobot.pkl', 'wb') as f:\n",
    "                pickle.dump([test_states, test_actions, test_rewards], f)\n",
    "            \n",
    "            images = []\n",
    "            for step in range(300):\n",
    "                observation = test_states[0, step].cpu().numpy()\n",
    "                env.set_state_(observation)\n",
    "                img = env.render(mode='rgb_array')\n",
    "\n",
    "                # Add text overlay directly to the image array\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.imshow(img)\n",
    "                ax.text(10, 30, f'Round: {rounds}, Episode: {episode}', color='white', fontsize=12, \n",
    "                        bbox=dict(facecolor='black', alpha=0.5))\n",
    "                ax.axis('off')\n",
    "                \n",
    "                fig.canvas.draw()\n",
    "                img_with_text = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "                img_with_text = img_with_text.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "                \n",
    "                images.append(img_with_text)\n",
    "                plt.close(fig)\n",
    "\n",
    "            if not os.path.exists('video_acrobot'):\n",
    "                os.makedirs('video_acrobot/')\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            imageio.mimsave('video_acrobot/round_{}_episode_{}.mp4'.format(rounds, episode), images)\n",
    "            env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "total_rounds = 1\n",
    "total_episodes = 250\n",
    "\n",
    "# Initialize lists to store x-axis values, mean rewards, and std deviations\n",
    "x_axis_values = []\n",
    "mean_rewards_list = []\n",
    "std_rewards_list = []\n",
    "\n",
    "# Loop over rounds and episodes\n",
    "for rounds in range(total_rounds):\n",
    "    for episode in range(0, total_episodes, 200):\n",
    "\n",
    "        policy_nn = Policy(env)\n",
    "        policy_nn.load_state_dict(torch.load('models/policy10/round_{}episode{}.pt'.format(rounds, episode)))\n",
    "        policy_nn.to(device)\n",
    "        policy_nn.eval()\n",
    "\n",
    "        us_V = basic_mdl(env.n, env.m, n_hid_layers=2, act=\"tanh\", n_hidden=200)\n",
    "        us_V.load_state_dict(torch.load('models/value10/round_{}episode{}.pt'.format(rounds, episode)))\n",
    "        us_V.to(device)\n",
    "        us_V.eval()\n",
    "\n",
    "        # List to collect rewards for different initial observations\n",
    "        rewards = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Htest,Ntest,Tup = 30,10,int(3.0/0.1)\n",
    "            initial_observations = [env.reset() for _ in range(10)]\n",
    "\n",
    "            for obs in initial_observations:\n",
    "                s0 = env.obs2state(torch.tensor(obs, device=device)).unsqueeze(0)\n",
    "                test_states, test_actions, test_rewards, _ = env.integrate_system(T=400, s0=s0, g=policy_nn)\n",
    "                true_test_rewards = test_rewards[...,Tup:].mean().item()\n",
    "                rewards.append(true_test_rewards)\n",
    "\n",
    "            # Calculate mean and std deviation of rewards\n",
    "            mean_reward = np.mean(rewards)\n",
    "            std_reward = np.std(rewards)\n",
    "\n",
    "            # Store the results\n",
    "            x_value = rounds + (episode / total_episodes)\n",
    "            x_axis_values.append(x_value)\n",
    "            mean_rewards_list.append(mean_reward)\n",
    "            std_rewards_list.append(std_reward)\n",
    "\n",
    "            print(f'Round: {rounds}, Episode: {episode}, X-axis Value: {x_value:.3f}, Mean Reward: {mean_reward:.3f}, Std Dev: {std_reward:.3f}')\n",
    "\n",
    "# Convert lists to numpy arrays for plotting\n",
    "x_axis_values = np.array(x_axis_values)\n",
    "mean_rewards_list = np.array(mean_rewards_list)\n",
    "std_rewards_list = np.array(std_rewards_list)\n",
    "\n",
    "# Define a soft blue color\n",
    "soft_blue = '#87CEEB'  # Light sky blue\n",
    "\n",
    "# Plotting the mean test rewards with shaded area for std deviation\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_axis_values, mean_rewards_list, linestyle='-', color=soft_blue, label='Mean Test Rewards')\n",
    "plt.fill_between(x_axis_values, \n",
    "                 mean_rewards_list - std_rewards_list, \n",
    "                 mean_rewards_list + std_rewards_list, \n",
    "                 color=soft_blue, alpha=0.3)\n",
    "\n",
    "# Customizing the plot\n",
    "plt.xlabel(\"Rounds\")\n",
    "plt.ylabel('Mean Test Rewards')\n",
    "plt.title(\"Test Performance\")\n",
    "plt.xticks(ticks=range(total_rounds + 1), labels=[f'{i}' for i in range(total_rounds + 1)])\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
